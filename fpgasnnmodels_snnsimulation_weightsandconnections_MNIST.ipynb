{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1F6ffvJDQGlRraeqZ3iPmZDAHhvnUQCDd",
      "authorship_tag": "ABX9TyMv41nIxh+FmAjiDkgoMz9N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SouvikUMD/Leaf-Disease-Prediction/blob/main/fpgasnnmodels_snnsimulation_weightsandconnections_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Background\n",
        "\n",
        "This is combined code of the software of FPGA-friendly SNN models inspired by\n",
        "the following link here [1]. The following code below follows the steps shown in [1]. First the network is simulated to get the approximate integer weights.\n",
        "Next, the weights are extracted from the network and converted into 16 bit hex values. Finally, the original weights are tested with the original test data and the classification accuracy is evaluated. PyTorch and BindsNET packages are utilized below. New Features add to this version are hidden neuron connections and outputs which will be used for the FPGA board. For this notebook, the network is simulated for integer weights and the weights are extracted for connections and converted into 16-bit hex values.\n",
        "\n",
        "[1] https://github.com/oshears/fpga_snn_models/tree/main."
      ],
      "metadata": {
        "id": "bawbtMzQ_LKx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set Up Environment"
      ],
      "metadata": {
        "id": "oCH4MxRK7ZWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_m_XGFjB7zGR",
        "outputId": "793ffa83-971f-4e59-df25-dae87a6251ef"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive/Colab Notebooks')"
      ],
      "metadata": {
        "id": "yUTh0bxl_tMd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Libraries"
      ],
      "metadata": {
        "id": "1LAEukaC7h-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install bindsnet"
      ],
      "metadata": {
        "id": "QQm1P9hD9-fr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls fpga_snn_models/models/*.py"
      ],
      "metadata": {
        "id": "ms95K5kICe06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74bd1b07-b5c8-4a57-96d2-bd3587c28a7f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fpga_snn_models/models/diehl_cook_snn.py  fpga_snn_models/models/__init__.py\n",
            "fpga_snn_models/models/if_snn.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat 'fpga_snn_models/models'"
      ],
      "metadata": {
        "id": "VnkXuW1hDJn6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd3fa0c0-663d-4a53-cce3-586d9e5dcba1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat: fpga_snn_models/models: Is a directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "path_name = '/content/drive/My Drive/Colab Notebooks/fpga_snn_models/'\n",
        "sys.path.append('fpga_snn_models/models')\n",
        "os.chdir(f'{path_name}models')"
      ],
      "metadata": {
        "id": "R1VkNruIDT3W"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "BindsNET is built on top of PyTorch platform. It is used for the simulation of SNNs and is geared towards machine learning and reinforcement learning. The BindsNet installation taken from [2]. Documentation Explanations for BindsNET is shown in [3].\n",
        "\n",
        "[2] https://github.com/BindsNET/bindsnet.\n",
        "\n",
        "[3] https://bindsnet-docs.readthedocs.io/"
      ],
      "metadata": {
        "id": "59EK3eHlmzbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/BindsNET/bindsnet.git"
      ],
      "metadata": {
        "id": "mbk-AiCUEqjZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c4cb454-9595-493f-fa49-e041266312bd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/BindsNET/bindsnet.git\n",
            "  Cloning https://github.com/BindsNET/bindsnet.git to /tmp/pip-req-build-j82b4w1u\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/BindsNET/bindsnet.git /tmp/pip-req-build-j82b4w1u\n",
            "  Resolved https://github.com/BindsNET/bindsnet.git to commit 0956fa8e029ef5e1e7257cb43ea2210a7260ce51\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting Cython<0.30.0,>=0.29.33 (from bindsnet==0.3.2)\n",
            "  Downloading Cython-0.29.37-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting foolbox<4.0.0,>=3.3.3 (from bindsnet==0.3.2)\n",
            "  Downloading foolbox-3.3.4-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gymnasium[accept-rom-license,atari]<0.30.0,>=0.29.1 (from bindsnet==0.3.2)\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from bindsnet==0.3.2) (3.7.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from bindsnet==0.3.2) (1.25.2)\n",
            "Requirement already satisfied: opencv-python<5.0.0.0,>=4.7.0.72 in /usr/local/lib/python3.10/dist-packages (from bindsnet==0.3.2) (4.8.0.76)\n",
            "Requirement already satisfied: pandas<3.0.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from bindsnet==0.3.2) (2.0.3)\n",
            "Collecting scikit-build<0.18.0,>=0.17.6 (from bindsnet==0.3.2)\n",
            "  Downloading scikit_build-0.17.6-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.3/84.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit-image<0.22.0,>=0.21.0 (from bindsnet==0.3.2)\n",
            "  Downloading scikit_image-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn<2.0.0,>=1.2.2 in /usr/local/lib/python3.10/dist-packages (from bindsnet==0.3.2) (1.2.2)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.10.1 in /usr/local/lib/python3.10/dist-packages (from bindsnet==0.3.2) (1.11.4)\n",
            "Collecting tensorboardX==2.6.2.2 (from bindsnet==0.3.2)\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch<3.0.0,>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from bindsnet==0.3.2) (2.2.1+cu121)\n",
            "Collecting torchaudio==2.1.2 (from bindsnet==0.3.2)\n",
            "  Downloading torchaudio-2.1.2-cp310-cp310-manylinux1_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.16.2 (from bindsnet==0.3.2)\n",
            "  Downloading torchvision-0.16.2-cp310-cp310-manylinux1_x86_64.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from bindsnet==0.3.2) (4.66.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX==2.6.2.2->bindsnet==0.3.2) (24.0)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX==2.6.2.2->bindsnet==0.3.2) (3.20.3)\n",
            "Collecting torch<3.0.0,>=2.1.2 (from bindsnet==0.3.2)\n",
            "  Downloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m964.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->bindsnet==0.3.2) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->bindsnet==0.3.2) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->bindsnet==0.3.2) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->bindsnet==0.3.2) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->bindsnet==0.3.2) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->bindsnet==0.3.2) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch<3.0.0,>=2.1.2->bindsnet==0.3.2)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch<3.0.0,>=2.1.2->bindsnet==0.3.2)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch<3.0.0,>=2.1.2->bindsnet==0.3.2)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch<3.0.0,>=2.1.2->bindsnet==0.3.2)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch<3.0.0,>=2.1.2->bindsnet==0.3.2)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch<3.0.0,>=2.1.2->bindsnet==0.3.2)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch<3.0.0,>=2.1.2->bindsnet==0.3.2)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch<3.0.0,>=2.1.2->bindsnet==0.3.2)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch<3.0.0,>=2.1.2->bindsnet==0.3.2)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.18.1 (from torch<3.0.0,>=2.1.2->bindsnet==0.3.2)\n",
            "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch<3.0.0,>=2.1.2->bindsnet==0.3.2)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Collecting triton==2.1.0 (from torch<3.0.0,>=2.1.2->bindsnet==0.3.2)\n",
            "  Downloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.16.2->bindsnet==0.3.2) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.16.2->bindsnet==0.3.2) (9.4.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch<3.0.0,>=2.1.2->bindsnet==0.3.2)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from foolbox<4.0.0,>=3.3.3->bindsnet==0.3.2) (67.7.2)\n",
            "Collecting eagerpy>=0.30.0 (from foolbox<4.0.0,>=3.3.3->bindsnet==0.3.2)\n",
            "  Downloading eagerpy-0.30.0-py3-none-any.whl (31 kB)\n",
            "Collecting GitPython>=3.0.7 (from foolbox<4.0.0,>=3.3.3->bindsnet==0.3.2)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]<0.30.0,>=0.29.1->bindsnet==0.3.2) (2.2.1)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium[accept-rom-license,atari]<0.30.0,>=0.29.1->bindsnet==0.3.2)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2 (from gymnasium[accept-rom-license,atari]<0.30.0,>=0.29.1->bindsnet==0.3.2)\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Collecting shimmy[atari]<1.0,>=0.1.0 (from gymnasium[accept-rom-license,atari]<0.30.0,>=0.29.1->bindsnet==0.3.2)\n",
            "  Downloading Shimmy-0.2.1-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.1->bindsnet==0.3.2) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.1->bindsnet==0.3.2) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.1->bindsnet==0.3.2) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.1->bindsnet==0.3.2) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.1->bindsnet==0.3.2) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.7.1->bindsnet==0.3.2) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0.0,>=2.0.2->bindsnet==0.3.2) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0.0,>=2.0.2->bindsnet==0.3.2) (2024.1)\n",
            "Requirement already satisfied: distro in /usr/lib/python3/dist-packages (from scikit-build<0.18.0,>=0.17.6->bindsnet==0.3.2) (1.7.0)\n",
            "Requirement already satisfied: tomli in /usr/local/lib/python3.10/dist-packages (from scikit-build<0.18.0,>=0.17.6->bindsnet==0.3.2) (2.0.1)\n",
            "Requirement already satisfied: wheel>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from scikit-build<0.18.0,>=0.17.6->bindsnet==0.3.2) (0.43.0)\n",
            "Requirement already satisfied: imageio>=2.27 in /usr/local/lib/python3.10/dist-packages (from scikit-image<0.22.0,>=0.21.0->bindsnet==0.3.2) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image<0.22.0,>=0.21.0->bindsnet==0.3.2) (2024.4.24)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image<0.22.0,>=0.21.0->bindsnet==0.3.2) (1.6.0)\n",
            "Requirement already satisfied: lazy_loader>=0.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image<0.22.0,>=0.21.0->bindsnet==0.3.2) (0.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0.0,>=1.2.2->bindsnet==0.3.2) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0.0,>=1.2.2->bindsnet==0.3.2) (3.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]<0.30.0,>=0.29.1->bindsnet==0.3.2) (8.1.7)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]<0.30.0,>=0.29.1->bindsnet==0.3.2)\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython>=3.0.7->foolbox<4.0.0,>=3.3.3->bindsnet==0.3.2)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib<4.0.0,>=3.7.1->bindsnet==0.3.2) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.16.2->bindsnet==0.3.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.16.2->bindsnet==0.3.2) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.16.2->bindsnet==0.3.2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.16.2->bindsnet==0.3.2) (2024.2.2)\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]<1.0,>=0.1.0->gymnasium[accept-rom-license,atari]<0.30.0,>=0.29.1->bindsnet==0.3.2)\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3.0.0,>=2.1.2->bindsnet==0.3.2) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<3.0.0,>=2.1.2->bindsnet==0.3.2) (1.3.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0->gymnasium[accept-rom-license,atari]<0.30.0,>=0.29.1->bindsnet==0.3.2) (6.4.0)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython>=3.0.7->foolbox<4.0.0,>=3.3.3->bindsnet==0.3.2)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: bindsnet, AutoROM.accept-rom-license\n",
            "  Building wheel for bindsnet (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bindsnet: filename=bindsnet-0.3.2-py3-none-any.whl size=118727 sha256=2756044f73e5c9240c9a396ec43867d7b7b2deed89481f3ad2465e2e7e88d13a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-h255v68a/wheels/f6/2c/e7/1d82e2755fef0a819200adf931214966655e8e69d6ae0442dc\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446659 sha256=8e4132540e9ea4d9266b22f8727988f073d2d1ff7d212abb2f992b7df6b54c99\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built bindsnet AutoROM.accept-rom-license\n",
            "Installing collected packages: farama-notifications, triton, tensorboardX, smmap, scikit-build, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, gymnasium, eagerpy, Cython, ale-py, shimmy, scikit-image, nvidia-cusparse-cu12, nvidia-cudnn-cu12, gitdb, AutoROM.accept-rom-license, autorom, nvidia-cusolver-cu12, GitPython, torch, foolbox, torchvision, torchaudio, bindsnet\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.2.0\n",
            "    Uninstalling triton-2.2.0:\n",
            "      Successfully uninstalled triton-2.2.0\n",
            "  Attempting uninstall: Cython\n",
            "    Found existing installation: Cython 3.0.10\n",
            "    Uninstalling Cython-3.0.10:\n",
            "      Successfully uninstalled Cython-3.0.10\n",
            "  Attempting uninstall: scikit-image\n",
            "    Found existing installation: scikit-image 0.19.3\n",
            "    Uninstalling scikit-image-0.19.3:\n",
            "      Successfully uninstalled scikit-image-0.19.3\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.2.1+cu121\n",
            "    Uninstalling torch-2.2.1+cu121:\n",
            "      Successfully uninstalled torch-2.2.1+cu121\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.17.1+cu121\n",
            "    Uninstalling torchvision-0.17.1+cu121:\n",
            "      Successfully uninstalled torchvision-0.17.1+cu121\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.2.1+cu121\n",
            "    Uninstalling torchaudio-2.2.1+cu121:\n",
            "      Successfully uninstalled torchaudio-2.2.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.17.1 requires torch==2.2.1, but you have torch 2.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed AutoROM.accept-rom-license-0.6.1 Cython-0.29.37 GitPython-3.1.43 ale-py-0.8.1 autorom-0.4.2 bindsnet-0.3.2 eagerpy-0.30.0 farama-notifications-0.0.4 foolbox-3.3.4 gitdb-4.0.11 gymnasium-0.29.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 scikit-build-0.17.6 scikit-image-0.21.0 shimmy-0.2.1 smmap-5.0.1 tensorboardX-2.6.2.2 torch-2.1.2 torchaudio-2.1.2 torchvision-0.16.2 triton-2.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy\n",
        "import numpy as np\n",
        "\n",
        "# import modules from pytorch\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "\n",
        "# import local modules\n",
        "from diehl_cook_snn import DiehlAndCookNetwork16,DiehlAndCookNetwork32\n",
        "from if_snn import IFNetwork16,IFNetwork32\n",
        "\n",
        "# miscellaneous imports\n",
        "import argparse\n",
        "\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from PIL import Image # PIL is the Python Imaging Library\n",
        "import random"
      ],
      "metadata": {
        "id": "e8fLcjhbLyVL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "F9HZSrly7Odj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb4fc5c8-6dd4-4712-fe37-9da86109597b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "# import modules from bindsnet\n",
        "from bindsnet.datasets import MNIST, DataLoader, CIFAR10\n",
        "from bindsnet.encoding import PoissonEncoder,BernoulliEncoder,RankOrderEncoder\n",
        "from bindsnet.evaluation import all_activity,proportion_weighting,assign_labels\n",
        "from bindsnet.network.monitors import Monitor\n",
        "from bindsnet.network import load"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Variable Setup."
      ],
      "metadata": {
        "id": "NaeMZOtUslA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create an argument parser to interpret command line arguments\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "# --encoding specifies the type of encoding (Poisson, Bernoulli or RankOrder)\n",
        "parser.add_argument(\"--encoding\", type=str, default=\"Poisson\")\n",
        "#parser.add_argument(\"--encoding\", type=str, default=\"Bernoulli\")\n",
        "#parser.add_argument(\"--encoding\", type=str, default=\"RankOrder\")\n",
        "parser.add_argument(\"--weight_size\", type=int, default=32)\n",
        "parser.add_argument(\"--neuron_type\", type=str, default=\"IF\")\n",
        "#parser.add_argument(\"--neuron_type\", type=str, default=\"diehlAndCook\")\n",
        "parser.add_argument(\"--batch_size\", type=int, default=64)\n",
        "\n",
        "# parse the arguments\n",
        "#args = parser.parse_args()\n",
        "args, unknown = parser.parse_known_args()\n",
        "\n",
        "\n",
        "# declare global variables\n",
        "\n",
        "# n_neurons specifies the number of neurons per layer\n",
        "n_neurons = 1\n",
        "\n",
        "# batch_size specifies the number of training samples to collect weight changes\n",
        "# from before updating the weights.\n",
        "batch_size = args.batch_size\n",
        "print(f\"Batch Size: {batch_size}\")\n",
        "\n",
        "# n_train specifies the number of training samples\n",
        "#n_train = 60000\n",
        "#n_train = 56000\n",
        "#n_train = 49000\n",
        "n_train = 5600\n",
        "\n",
        "# n_test specifies the number of testing samples\n",
        "#n_test = 10000\n",
        "#n_test = 14000\n",
        "#n_test = 21000\n",
        "n_test = 1400\n",
        "\n",
        "# update_steps specifies the number of batches to process before reporting\n",
        "# an update.\n",
        "update_steps = 10\n",
        "\n",
        "# time specifies the simulation time of the SNN\n",
        "time = 100\n",
        "\n",
        "# dt specifies the timestep size for the simulation time\n",
        "dt = 1\n",
        "\n",
        "# intensity specifies the maximum intensity of the input data\n",
        "intensity = 128\n",
        "\n",
        "# gpu setting\n",
        "gpu = torch.cuda.is_available()\n",
        "print(f\"GPU: {gpu}\")\n",
        "\n",
        "# Update_interval specifies the number of samples processed before updating\n",
        "# accuracy estimations.\n",
        "update_interval = update_steps * batch_size\n",
        "\n",
        "# Setup CUDA\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() and gpu else \"cpu\")\n",
        "\n",
        "# Determine number of worker threads to load data.\n",
        "n_workers = 0\n",
        "# if n_workers == -1:\n",
        "#     n_workers = gpu * 4 * torch.cuda.device_count()\n",
        "\n",
        "# Report the selected encoding scheme, neural model and learning technique.\n",
        "print(\"Encoding Scheme:\",args.encoding)\n",
        "\n",
        "# Assign a value to the encoder based on the input argument.\n",
        "encoder = None\n",
        "if args.encoding == \"Poisson\":\n",
        "    encoder = PoissonEncoder(time=time,dt=dt)\n",
        "if args.encoding == \"Bernoulli\":\n",
        "    encoder = BernoulliEncoder(time=time,dt=dt)\n",
        "if args.encoding == \"RankOrder\":\n",
        "    encoder = RankOrderEncoder(time=time,dt=dt)\n",
        "\n",
        "# Build network based on the input argument.\n",
        "print(f\"Neuron Model: {args.neuron_type}\")\n",
        "print(f\"Weight Size: {args.weight_size}\")\n",
        "\n",
        "# Set up dimensions and pixel size.\n",
        "dim = 28\n",
        "pixel_size = 1\n",
        "\n",
        "neuron_type = \"\"\n",
        "if args.neuron_type == \"IF\":\n",
        "    neuron_type = \"if\"\n",
        "    if args.weight_size == 16:\n",
        "        network = IFNetwork16(n_inpt=dim**2,\n",
        "                              inpt_shape=(pixel_size, dim, dim), batch_size=128,\n",
        "                              n_neurons=n_neurons)\n",
        "    elif args.weight_size == 32:\n",
        "        network = IFNetwork32(n_inpt=pixel_size*dim**2,\n",
        "                              inpt_shape=(pixel_size, dim, dim), batch_size=128,\n",
        "                              n_neurons=n_neurons)\n",
        "else:\n",
        "    neuron_type = \"diehlAndCook\"\n",
        "    if args.weight_size == 16:\n",
        "      network = DiehlAndCookNetwork16(n_inpt=pixel_size*dim**2,\n",
        "                                      inpt_shape=(pixel_size, dim, dim),\n",
        "                                      batch_size=128,\n",
        "                                      n_neurons=n_neurons)\n",
        "    elif args.weight_size == 32:\n",
        "        network = DiehlAndCookNetwork32(n_inpt=pixel_size*dim**2,\n",
        "                                        inpt_shape=(pixel_size, dim, dim),\n",
        "                                        batch_size=128,\n",
        "                                        n_neurons=n_neurons)"
      ],
      "metadata": {
        "id": "f3YF47dqf1q_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a33a6618-fa18-4cc8-8260-7f5a268b90d4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Size: 64\n",
            "GPU: False\n",
            "Encoding Scheme: Poisson\n",
            "Neuron Model: IF\n",
            "Weight Size: 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Training and Testing Dataset."
      ],
      "metadata": {
        "id": "kWBugNEUsxKp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The BindsNET package also comes with different datasets like CIFAR10, ALOV, and MNIST. This code can handle CIFAR10 dataset and MNIST dataset. The dimensions of an MNIST image are 28x28 pixels while the dimensions of a CIFAR10 dataset are 32x32. Before proceeding further beyond the code, please change dim to either 28 for access to MNIST datasets or 32 for CIFAR10 datasets. in the Variable Setup section located above."
      ],
      "metadata": {
        "id": "_BGZmIifBKCX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the MNIST dataset.\n",
        "train_dataset = MNIST(\n",
        "    encoder,\n",
        "    None,\n",
        "    root=os.path.join(\".\", \"data\", \"MNIST\"),\n",
        "    download=True,\n",
        "    transform=transforms.Compose(\n",
        "        [transforms.ToTensor(), transforms.Lambda(lambda x: x * intensity)]\n",
        "    ),\n",
        ")\n",
        "\n",
        "test_dataset = MNIST(\n",
        "    encoder,\n",
        "    None,\n",
        "    root=os.path.join(\".\", \"data\", \"MNIST\"),\n",
        "    download=True,\n",
        "    train=False,\n",
        "    transform=transforms.Compose(\n",
        "        [transforms.ToTensor(), transforms.Lambda(lambda x: x * intensity)]\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Pull n_test random elements from test_dataset.\n",
        "train_dataset_rnd = random.sample(list(train_dataset.data), n_train)\n",
        "#train_dataset_rnd = resize_images(train_dataset_rnd)\n",
        "print(len(train_dataset_rnd))\n",
        "train_dataset.data = train_dataset_rnd\n",
        "print(\"Random sample of Train Data:\", train_dataset_rnd)\n",
        "\n",
        "# Create a dataloader to iterate over and batch the training data.\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size,\n",
        "                              shuffle=True, num_workers=n_workers,\n",
        "                              pin_memory=True)\n",
        "\n",
        "# Pull n_test random elements from test_dataset\n",
        "test_dataset_rnd = random.sample(list(test_dataset.data), n_test)\n",
        "#test_dataset_rnd = resize_images(test_dataset_rnd)\n",
        "test_dataset.data = test_dataset_rnd\n",
        "print(\"Random sample of Test Data:\", test_dataset_rnd)\n",
        "\n",
        "# Create a dataloader to iterate over and batch the test data.\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
        "                            num_workers=n_workers, pin_memory=True)"
      ],
      "metadata": {
        "id": "q0fUUQWD_O_E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "273359a5-0413-4040-8c25-0217e6f5aaeb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5600\n",
            "Random sample of Train Data: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Declare variables needed for estimating the network accuracy."
      ],
      "metadata": {
        "id": "hb-eAu2dvPDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_classes = 10\n",
        "\n",
        "# assignments stores the label that each output neuron corresponds to\n",
        "assignments = -torch.ones(n_neurons, device=device)\n",
        "\n",
        "# proportions stores the ratio of the number of times each of the output neurons\n",
        "# produced a spike for the corresponding class relative to other classes.\n",
        "proportions = torch.zeros((n_neurons, n_classes), device=device)\n",
        "\n",
        "# rates stores the number of times each of the output neurons produced a spike\n",
        "# for the corresponding class.\n",
        "rates = torch.zeros((n_neurons, n_classes), device=device)\n",
        "\n",
        "# create a dictionary to store all assignment and proportional assignment\n",
        "# accuracy values.\n",
        "accuracy = {\"all\": [], \"proportion\": []}\n",
        "\n",
        "# create a monitor to record the spiking activity of the output layer (Y).\n",
        "output_spikes_monitor = Monitor(network.layers[\"Y\"], state_vars=[\"s\"],\n",
        "                                time=int(time / dt))\n",
        "\n",
        "# add the monitor to the network\n",
        "network.add_monitor(output_spikes_monitor, name=\"Y\")\n",
        "\n",
        "# create a tensor to store the spiking activity for all neurons for the\n",
        "# duration of the update_interval.\n",
        "spike_record = torch.zeros((update_interval, int(time / dt),\n",
        "                            n_neurons), device=device)"
      ],
      "metadata": {
        "id": "WDQaZj79RdiM"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SNN Simulation"
      ],
      "metadata": {
        "id": "w1fEA4xbwOwK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Network with Training Dataset."
      ],
      "metadata": {
        "id": "aSq3vinCs3_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train the network\n",
        "print(\"\\nBegin training.\\n\")\n",
        "\n",
        "# create a list to store the sample labels for each batch in the update interval\n",
        "labels = []\n",
        "\n",
        "# iterate through each batch of data\n",
        "for step, batch in enumerate(train_dataloader):\n",
        "    # get next input sample\n",
        "    inputs = {\"X\": batch[\"encoded_image\"]}\n",
        "    if gpu:\n",
        "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
        "\n",
        "    # if it is time to print out an accuracy estimate\n",
        "    if step % update_steps == 0 and step > 0:\n",
        "\n",
        "        # convert the array of labels into a tensor\n",
        "        label_tensor = torch.tensor(labels, device=device)\n",
        "\n",
        "        # get network predictions based on the spiking activity, previous\n",
        "        # assignments and number of classes.\n",
        "        all_activity_pred = all_activity(spikes=spike_record,\n",
        "                                        assignments=assignments,\n",
        "                                        n_labels=n_classes)\n",
        "\n",
        "        # get network predictions based on the spiking activity, previous\n",
        "        # assignments, proportional assignments and number of classes.\n",
        "        proportion_pred = proportion_weighting(spikes=spike_record,\n",
        "                                              assignments=assignments,\n",
        "                                              proportions=proportions,\n",
        "                                              n_labels=n_classes)\n",
        "\n",
        "        # compute the network accuracy based on the prediction results and\n",
        "        # append to the assignment accuracy dictionary.\n",
        "        accuracy[\"all\"].append(100 * torch.sum(label_tensor.long() ==\n",
        "                                              all_activity_pred).item() /\n",
        "                              len(label_tensor))\n",
        "\n",
        "        # compute the network accuracy based on the proportional prediction\n",
        "        # results and append to the assignment accuracy dictionary.\n",
        "        accuracy[\"proportion\"].append(100 * torch.sum(label_tensor.long() ==\n",
        "                                                      proportion_pred).item() /\n",
        "                                      len(label_tensor))\n",
        "\n",
        "        # report the network accuracy at the current time\n",
        "        print(\"\\nAll activity accuracy: %.2f (last), %.2f (average), %.2f (best)\"\n",
        "              % (accuracy[\"all\"][-1], np.mean(accuracy[\"all\"]),\n",
        "                np.max(accuracy[\"all\"])))\n",
        "        print(\"Proportion weighting accuracy: %.2f (last), %.2f (average), %.2f\" \" (best)\"\n",
        "              % (accuracy[\"proportion\"][-1], np.mean(accuracy[\"proportion\"]),\n",
        "                np.max(accuracy[\"proportion\"])))\n",
        "\n",
        "        # display how many samples are remaining\n",
        "        print(\"Progress:\",step*batch_size,\"/\",n_train)\n",
        "\n",
        "        # update the neuron assignments, proportional assignments and spiking\n",
        "        # rates.\n",
        "        assignments, proportions, rates = assign_labels(spikes=spike_record,\n",
        "                                                        labels=label_tensor,\n",
        "                                                        n_labels=n_classes,\n",
        "                                                        rates=rates)\n",
        "\n",
        "        # reset the list of labels\n",
        "        labels = []\n",
        "\n",
        "    # append the labels of the current batch to the list of labels\n",
        "    labels.extend(batch[\"label\"].tolist())\n",
        "\n",
        "    # run the network on the input\n",
        "    network.run(inputs=inputs, time=time, input_time_dim=0.5)\n",
        "\n",
        "    # get the spikes produced by the current batch\n",
        "    s = output_spikes_monitor.get(\"s\").permute((1, 0, 2))\n",
        "\n",
        "    # store the spikes inside of the spike record list at the current batch's\n",
        "    # index (relative to the number of batches in the update interval).\n",
        "    spike_record[(step * batch_size) % update_interval : (step * batch_size %\n",
        "                                                          update_interval) +\n",
        "                s.size(0)] = s\n",
        "\n",
        "    # reset the network before running it again\n",
        "    network.reset_state_variables()\n",
        "\n",
        "print(\"Training complete.\\n\")"
      ],
      "metadata": {
        "id": "fky9UbfMQMPv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18fafd29-7a44-44ff-87da-fe06e729a895"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Begin training.\n",
            "\n",
            "\n",
            "All activity accuracy: 10.62 (last), 10.62 (average), 10.62 (best)\n",
            "Proportion weighting accuracy: 10.62 (last), 10.62 (average), 10.62 (best)\n",
            "Progress: 640 / 5600\n",
            "\n",
            "All activity accuracy: 10.00 (last), 10.31 (average), 10.62 (best)\n",
            "Proportion weighting accuracy: 10.00 (last), 10.31 (average), 10.62 (best)\n",
            "Progress: 1280 / 5600\n",
            "\n",
            "All activity accuracy: 10.47 (last), 10.36 (average), 10.62 (best)\n",
            "Proportion weighting accuracy: 10.47 (last), 10.36 (average), 10.62 (best)\n",
            "Progress: 1920 / 5600\n",
            "\n",
            "All activity accuracy: 10.00 (last), 10.27 (average), 10.62 (best)\n",
            "Proportion weighting accuracy: 10.00 (last), 10.27 (average), 10.62 (best)\n",
            "Progress: 2560 / 5600\n",
            "\n",
            "All activity accuracy: 10.00 (last), 10.22 (average), 10.62 (best)\n",
            "Proportion weighting accuracy: 10.00 (last), 10.22 (average), 10.62 (best)\n",
            "Progress: 3200 / 5600\n",
            "\n",
            "All activity accuracy: 9.06 (last), 10.03 (average), 10.62 (best)\n",
            "Proportion weighting accuracy: 9.06 (last), 10.03 (average), 10.62 (best)\n",
            "Progress: 3840 / 5600\n",
            "\n",
            "All activity accuracy: 8.91 (last), 9.87 (average), 10.62 (best)\n",
            "Proportion weighting accuracy: 8.91 (last), 9.87 (average), 10.62 (best)\n",
            "Progress: 4480 / 5600\n",
            "\n",
            "All activity accuracy: 11.88 (last), 10.12 (average), 11.88 (best)\n",
            "Proportion weighting accuracy: 11.88 (last), 10.12 (average), 11.88 (best)\n",
            "Progress: 5120 / 5600\n",
            "Training complete.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save the Network."
      ],
      "metadata": {
        "id": "dN0MfnmE2Qzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "networkFile = f\"{path_name}networks/{neuron_type}_{args.encoding}_{batch_size}\\\n",
        "_{args.weight_size}bit_snn.pt\"\n",
        "#filename = f\"{path_name}networks/{neuron_type}_{args.encoding}_{batch_size}_{args.weight_size}bit_snn.pt\"\n",
        "#network.save(filename)\n",
        "network.save(networkFile)\n",
        "\n",
        "\n",
        "# write out network assignments and proportions\n",
        "assignmentFile = f'{path_name}/networks/{neuron_type}_{args.encoding}_{batch_size}_{args.weight_size}bit_snn_assignments.pt'\n",
        "torch.save(assignments, assignmentFile)\n",
        "proportionsFile = f'{path_name}/networks/{neuron_type}_{args.encoding}_{batch_size}_{args.weight_size}bit_snn_proportions.pt'\n",
        "torch.save(proportions, proportionsFile)\n",
        "\n",
        "# create a dictionary to store all assignment and proportional assignment accuracy values for the test data\n",
        "accuracy = {\"all\": 0, \"proportion\": 0}"
      ],
      "metadata": {
        "id": "KKzkl1o_HWxa"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Network with Testing Dataset."
      ],
      "metadata": {
        "id": "9s9QBPaBs88x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save the network\n",
        "filename = f\"{path_name}networks/{neuron_type}_{args.encoding}_{batch_size}_{args.weight_size}bit_snn.pt\"\n",
        "network.save(filename)\n",
        "\n",
        "# write out network assignments and proportions\n",
        "torch.save(assignments,f'{path_name}/networks/{neuron_type}_{args.encoding}_{batch_size}_{args.weight_size}bit_snn_assignments.pt')\n",
        "torch.save(proportions,f'{path_name}/networks/{neuron_type}_{args.encoding}_{batch_size}_{args.weight_size}bit_snn_proportions.pt')\n",
        "\n",
        "# create a dictionary to store all assignment and proportional assignment accuracy values for the test data\n",
        "accuracy = {\"all\": 0, \"proportion\": 0}\n",
        "\n",
        "# run the network for each test sample\n",
        "print(\"\\nBegin testing\\n\")\n",
        "\n",
        "# put the network into test mode\n",
        "network.train(mode=False)\n",
        "\n",
        "# iterate over each batch\n",
        "for step, batch in enumerate(test_dataloader):\n",
        "\n",
        "    # get next input sample and send to the GPU\n",
        "    if (device.type == 'gpu'):\n",
        "      inputs = {\"X\": batch[\"encoded_image\"].cuda()}\n",
        "    else:\n",
        "      inputs = {\"X\": batch[\"encoded_image\"]}\n",
        "\n",
        "    # run the network on the input\n",
        "    network.run(inputs=inputs, time=time, input_time_dim=25)\n",
        "\n",
        "    # get the spikes produced by the current batch\n",
        "    spike_record = output_spikes_monitor.get(\"s\").permute((1, 0, 2))\n",
        "\n",
        "    # convert the array of labels into a tensor\n",
        "    label_tensor = torch.tensor(batch[\"label\"], device=device)\n",
        "\n",
        "    # get network predictions based on the spiking activity,\n",
        "    # previous assignments and number of classes\n",
        "    if (device.type == 'gpu'):\n",
        "      all_activity_pred = all_activity(spikes=spike_record.\n",
        "                                       to(torch.device('cuda')),\n",
        "                                      assignments=assignments,\n",
        "                                      n_labels=n_classes)\n",
        "    else:\n",
        "      all_activity_pred = all_activity(spikes=spike_record,\n",
        "                                     assignments=assignments,\n",
        "                                     n_labels=n_classes)\n",
        "\n",
        "    # get network predictions based on the spiking activity, previous\n",
        "    # assignments, proportional assignments and number of classes\n",
        "    if (device.type == 'gpu'):\n",
        "      proportion_pred = proportion_weighting(spikes=spike_record.\n",
        "                                             to(torch.device('cuda')),\n",
        "                                           assignments=assignments,\n",
        "                                            proportions=proportions,\n",
        "                                            n_labels=n_classes)\n",
        "    else:\n",
        "      proportion_pred = proportion_weighting(spikes=spike_record,\n",
        "                                      assignments=assignments,\n",
        "                                      proportions=proportions,\n",
        "                                      n_labels=n_classes)\n",
        "\n",
        "\n",
        "    # compute the network accuracy based on the prediction results and add the\n",
        "    # results to the accuracy dictionary\n",
        "    accuracy[\"all\"] += float(torch.sum(label_tensor.long() ==\n",
        "                                       all_activity_pred).item())\n",
        "\n",
        "    # compute the network accuracy based on the proportional prediction results\n",
        "    # and add the results to the accuracy dictionary\n",
        "    accuracy[\"proportion\"] += float(torch.sum(label_tensor.long() ==\n",
        "                                               proportion_pred).item())\n",
        "\n",
        "    # if it is time to print out an accuracy estimate\n",
        "    if step % update_steps == 0 and step > 0:\n",
        "        # print out the assignment and proportional assignment accuracy\n",
        "        print(\"\\nAll activity accuracy: %.2f\" % (accuracy[\"all\"] / n_test))\n",
        "        print(\"Proportion weighting accuracy: %.2f\"\n",
        "              % (accuracy[\"proportion\"] / n_test))\n",
        "\n",
        "        #print out how many test samples are remaining\n",
        "        print(\"Progress:\",step*batch_size,\"/\",n_test)\n",
        "\n",
        "    # reset the network before running it again\n",
        "    network.reset_state_variables()\n",
        "\n",
        "# print out the final assignment and proportional assignment accuracies\n",
        "print(\"\\nAll activity accuracy: %.2f\" % (accuracy[\"all\"] / n_test))\n",
        "print(\"Proportion weighting accuracy: %.2f \\n\"\n",
        "      % (accuracy[\"proportion\"] / n_test))\n",
        "print(\"Testing complete.\\n\")"
      ],
      "metadata": {
        "id": "DvTLVok2zeQb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0483d6a6-be99-48cb-ce34-b9d51a4bab4b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Begin testing\n",
            "\n",
            "\n",
            "All activity accuracy: 0.05\n",
            "Proportion weighting accuracy: 0.05\n",
            "Progress: 640 / 1400\n",
            "\n",
            "All activity accuracy: 0.10\n",
            "Proportion weighting accuracy: 0.10\n",
            "Progress: 1280 / 1400\n",
            "\n",
            "All activity accuracy: 0.10\n",
            "Proportion weighting accuracy: 0.10 \n",
            "\n",
            "Testing complete.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract Weights and Connections"
      ],
      "metadata": {
        "id": "2QSgD1vSwX9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hexSize = args.weight_size / 4\n",
        "hexSize = int(16 / 4)\n",
        "\n",
        "#networkFile = f\"{path_name}networks/if_Poisson_1_32bit_snn.pt\"\n",
        "#networkFile = f\"{path_name}networks/if_Poisson_64_32bit_snn.pt\"\n",
        "#weightFileDirectory = f\"{path_name}networks/if_Poisson_1_32bit_weights\"\n",
        "#weightFileDirectory = f\"{path_name}networks/if_Poisson_64_32bit_weights\"\n",
        "weightFileDirectory = f\"{path_name}networks/{neuron_type}_{args.encoding}_\\\n",
        "{batch_size}_{args.weight_size}bit_weights\"\n",
        "\n",
        "#weightFileDirectory = f\"{path_name}networks/if_Poisson_64_32bit_weights_onelayer\"\n",
        "#weightFileDirectory = f\"{path_name}networks/if_Poisson_64_32bit_weights_25layers\"\n",
        "\n",
        "network = None\n",
        "\n",
        "if gpu:\n",
        "  #network = network.load(f\"{path_name}networks/if_Poisson_1_32bit_snn.pt\")\n",
        "  network = load(networkFile)\n",
        "  #network = load(f\"{path_name}networks/if_Poisson_64_32bit_snn.pt\")\n",
        "\n",
        "else:\n",
        "  #network = network.load(f\"{path_name}networks/if_Poisson_1_32bit_snn.pt\", map_location=torch.device('cpu'))\n",
        "  network = load(networkFile, map_location=torch.device('cpu'))\n",
        "  #network = load(f\"{path_name}networks/if_Poisson_64_32bit_snn.pt\",\n",
        "  #               map_location=torch.device('cpu'))\n",
        "\n",
        "\n",
        "\n",
        "# extract connections\n",
        "excitatoryConnectionWeights = network.connections[\"X\",\"Y\"].w\n",
        "\n",
        "#inhibitoryConnectionWeights = network.connections[\"Y\",\"Y\"].w\n",
        "\n",
        "# print(f\"Excitatory Connections: { excitatoryConnection.w } \")\n",
        "# print(f\"Inhibitory Connections: { inhibitoryConnection.w } \")"
      ],
      "metadata": {
        "id": "WqmmPldswbdT"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Adjusted Weights and Create Connections."
      ],
      "metadata": {
        "id": "4zMlSyz56uc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for each hidden layer neuron\n",
        "for neuronIdx in range(excitatoryConnectionWeights.shape[1]):\n",
        "#for neuronIdx in range(inhibitoryConnectionWeights.shape[1]):\n",
        "    # new file\n",
        "    neuronFile = open(f\"{weightFileDirectory}/{neuronIdx}.txt\",\"w\")\n",
        "    neuronFile_Inputs = open(f\"{weightFileDirectory}/{neuronIdx}_in_conn.txt\",\n",
        "                             \"w\")\n",
        "    print(f\"Weights for {neuronIdx}: {excitatoryConnectionWeights[neuronIdx]}\")\n",
        "    # for each input neuron\n",
        "    for inputIdx in range(excitatoryConnectionWeights.shape[0]):\n",
        "    #for inputIdx in range(inhibitoryConnectionWeights.shape[0]):\n",
        "        # write the weight values to the neuronFile\n",
        "        neuronFile_Inputs.write(f\"{neuronIdx}-{inputIdx}: \")\n",
        "        # write neuron input connections to neuronFile_inputs file using the\n",
        "        # following format {layer index}-{input index}\n",
        "        weightValue = int(excitatoryConnectionWeights[inputIdx][neuronIdx].\n",
        "                            numpy())\n",
        "        if (neuronIdx > 0):\n",
        "          weightValue_prev = int(excitatoryConnectionWeights[inputIdx][neuronIdx - 1].\n",
        "                            numpy())\n",
        "          for j in range(excitatoryConnectionWeights.shape[0]):\n",
        "            if (weightValue_prev != 0):\n",
        "                neuronFile_Inputs.write(f\"{neuronIdx - 1}-{j} \")\n",
        "\n",
        "        if (neuronIdx < excitatoryConnectionWeights.shape[1] - 1):\n",
        "          weightValue_next = int(excitatoryConnectionWeights[inputIdx][neuronIdx + 1].numpy())\n",
        "          for j in range(excitatoryConnectionWeights.shape[0]):\n",
        "            if (weightValue_next != 0):\n",
        "              neuronFile_Inputs.write(f\"{neuronIdx + 1}-{j} \")\n",
        "\n",
        "\n",
        "\n",
        "        neuronFile_Inputs.write(f\"\\n\")\n",
        "\n",
        "\n",
        "        #weightValue = int(inhibitoryConnectionWeights[inputIdx][neuronIdx].numpy())\n",
        "        #print(\"weightValue: \", weightValue)\n",
        "        hexWeightValue = hex(weightValue)[2:].zfill(hexSize).upper()\n",
        "        #print(\"hexWeightValue: \", hexWeightValue)\n",
        "        #neuronFile.write(f\"{inputIdx}-{weightValue}({hexWeightValue})\\n\")\n",
        "        neuronFile.write(f\"{hexWeightValue}\\n\")\n",
        "        #neuronFileNum.write(f\"{excitatoryConnectionWeights[inputIdx][neuronIdx].numpy()}\\n\")\n",
        "\n",
        "    neuronFile.close()\n",
        "    neuronFile_Inputs.close()\n",
        "    #neuronFileNum.close()"
      ],
      "metadata": {
        "id": "4_VD1UUOztnz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "254ccbfc-9702-44d3-d068-efa64390d76d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights for 0: tensor([0.])\n"
          ]
        }
      ]
    }
  ]
}