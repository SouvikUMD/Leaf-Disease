<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url(https://themes.googleusercontent.com/fonts/css?kit=fpjTOVmNbO4Lz34iLyptLSGpxL_7AiMlXksihSwXLVVhBSK8TwnmX7_MxI-MEYS8BRFKUaAHKqnvhGgz58RGvDF1cKUhszjoajnOYVFiWwS4s32I7Hx_V2ng6ZkSmCTeW1dFrxNXBMgKCKucmGZUnA);ul.lst-kix_wgrv86mrt3st-0{list-style-type:none}ul.lst-kix_wgrv86mrt3st-1{list-style-type:none}ol.lst-kix_z664cauv64el-2.start{counter-reset:lst-ctn-kix_z664cauv64el-2 0}ul.lst-kix_wgrv86mrt3st-6{list-style-type:none}ul.lst-kix_wgrv86mrt3st-7{list-style-type:none}ul.lst-kix_wgrv86mrt3st-8{list-style-type:none}ul.lst-kix_wgrv86mrt3st-2{list-style-type:none}ul.lst-kix_wgrv86mrt3st-3{list-style-type:none}ul.lst-kix_wgrv86mrt3st-4{list-style-type:none}ul.lst-kix_wgrv86mrt3st-5{list-style-type:none}ol.lst-kix_vnleep2i0q00-3.start{counter-reset:lst-ctn-kix_vnleep2i0q00-3 0}.lst-kix_z664cauv64el-6>li{counter-increment:lst-ctn-kix_z664cauv64el-6}.lst-kix_c0z30dja5xqc-6>li{counter-increment:lst-ctn-kix_c0z30dja5xqc-6}.lst-kix_2o2vvckau1te-0>li:before{content:"-  "}.lst-kix_2o2vvckau1te-1>li:before{content:"-  "}.lst-kix_2o2vvckau1te-3>li:before{content:"-  "}.lst-kix_2o2vvckau1te-2>li:before{content:"-  "}.lst-kix_2o2vvckau1te-4>li:before{content:"-  "}.lst-kix_c0z30dja5xqc-8>li{counter-increment:lst-ctn-kix_c0z30dja5xqc-8}ul.lst-kix_c0z30dja5xqc-1{list-style-type:none}.lst-kix_2o2vvckau1te-7>li:before{content:"-  "}.lst-kix_ek9djv3fjw0j-0>li:before{content:"-  "}.lst-kix_ek9djv3fjw0j-1>li:before{content:"-  "}.lst-kix_2o2vvckau1te-6>li:before{content:"-  "}.lst-kix_2o2vvckau1te-8>li:before{content:"-  "}.lst-kix_2o2vvckau1te-5>li:before{content:"-  "}.lst-kix_ek9djv3fjw0j-2>li:before{content:"-  "}.lst-kix_ek9djv3fjw0j-3>li:before{content:"-  "}ol.lst-kix_z664cauv64el-8.start{counter-reset:lst-ctn-kix_z664cauv64el-8 0}ol.lst-kix_c0z30dja5xqc-3.start{counter-reset:lst-ctn-kix_c0z30dja5xqc-3 0}.lst-kix_c0z30dja5xqc-4>li{counter-increment:lst-ctn-kix_c0z30dja5xqc-4}ol.lst-kix_vnleep2i0q00-8.start{counter-reset:lst-ctn-kix_vnleep2i0q00-8 0}ol.lst-kix_z664cauv64el-3.start{counter-reset:lst-ctn-kix_z664cauv64el-3 0}.lst-kix_vnleep2i0q00-1>li:before{content:"\0025cb   "}.lst-kix_vnleep2i0q00-0>li:before{content:"" counter(lst-ctn-kix_vnleep2i0q00-0,decimal) ". "}.lst-kix_vnleep2i0q00-4>li:before{content:"" counter(lst-ctn-kix_vnleep2i0q00-4,lower-latin) ". "}.lst-kix_vnleep2i0q00-3>li:before{content:"" counter(lst-ctn-kix_vnleep2i0q00-3,decimal) ". "}.lst-kix_vnleep2i0q00-2>li:before{content:"" counter(lst-ctn-kix_vnleep2i0q00-2,lower-roman) ". "}ol.lst-kix_c0z30dja5xqc-8.start{counter-reset:lst-ctn-kix_c0z30dja5xqc-8 0}ul.lst-kix_1oaclixybewz-0{list-style-type:none}.lst-kix_vnleep2i0q00-7>li:before{content:"" counter(lst-ctn-kix_vnleep2i0q00-7,lower-latin) ". "}ul.lst-kix_1oaclixybewz-3{list-style-type:none}.lst-kix_z664cauv64el-4>li{counter-increment:lst-ctn-kix_z664cauv64el-4}ul.lst-kix_1oaclixybewz-4{list-style-type:none}ul.lst-kix_1oaclixybewz-1{list-style-type:none}ul.lst-kix_1oaclixybewz-2{list-style-type:none}.lst-kix_vnleep2i0q00-8>li:before{content:"" counter(lst-ctn-kix_vnleep2i0q00-8,lower-roman) ". "}ul.lst-kix_1oaclixybewz-7{list-style-type:none}ul.lst-kix_1oaclixybewz-8{list-style-type:none}ul.lst-kix_1oaclixybewz-5{list-style-type:none}ul.lst-kix_1oaclixybewz-6{list-style-type:none}.lst-kix_vnleep2i0q00-5>li:before{content:"" counter(lst-ctn-kix_vnleep2i0q00-5,lower-roman) ". "}.lst-kix_vnleep2i0q00-6>li:before{content:"" counter(lst-ctn-kix_vnleep2i0q00-6,decimal) ". "}ul.lst-kix_ek9djv3fjw0j-3{list-style-type:none}ul.lst-kix_ek9djv3fjw0j-2{list-style-type:none}ul.lst-kix_ek9djv3fjw0j-5{list-style-type:none}.lst-kix_c0z30dja5xqc-4>li:before{content:"" counter(lst-ctn-kix_c0z30dja5xqc-4,lower-latin) ". "}.lst-kix_c0z30dja5xqc-6>li:before{content:"" counter(lst-ctn-kix_c0z30dja5xqc-6,decimal) ". "}ul.lst-kix_ek9djv3fjw0j-4{list-style-type:none}ul.lst-kix_ek9djv3fjw0j-7{list-style-type:none}ul.lst-kix_ek9djv3fjw0j-6{list-style-type:none}ul.lst-kix_ek9djv3fjw0j-8{list-style-type:none}.lst-kix_vnleep2i0q00-7>li{counter-increment:lst-ctn-kix_vnleep2i0q00-7}.lst-kix_c0z30dja5xqc-2>li:before{content:"" counter(lst-ctn-kix_c0z30dja5xqc-2,lower-roman) ". "}ul.lst-kix_ek9djv3fjw0j-1{list-style-type:none}ul.lst-kix_ek9djv3fjw0j-0{list-style-type:none}ol.lst-kix_z664cauv64el-6.start{counter-reset:lst-ctn-kix_z664cauv64el-6 0}.lst-kix_z664cauv64el-3>li:before{content:"" counter(lst-ctn-kix_z664cauv64el-3,decimal) ". "}.lst-kix_c0z30dja5xqc-2>li{counter-increment:lst-ctn-kix_c0z30dja5xqc-2}.lst-kix_z664cauv64el-0>li{counter-increment:lst-ctn-kix_z664cauv64el-0}.lst-kix_z664cauv64el-5>li:before{content:"" counter(lst-ctn-kix_z664cauv64el-5,lower-roman) ". "}.lst-kix_z664cauv64el-7>li:before{content:"" counter(lst-ctn-kix_z664cauv64el-7,lower-latin) ". "}.lst-kix_c0z30dja5xqc-0>li:before{content:"" counter(lst-ctn-kix_c0z30dja5xqc-0,decimal) ". "}.lst-kix_50nrsf1v7avc-2>li:before{content:"-  "}ol.lst-kix_vnleep2i0q00-5.start{counter-reset:lst-ctn-kix_vnleep2i0q00-5 0}.lst-kix_50nrsf1v7avc-4>li:before{content:"-  "}.lst-kix_c0z30dja5xqc-3>li{counter-increment:lst-ctn-kix_c0z30dja5xqc-3}ol.lst-kix_vnleep2i0q00-2.start{counter-reset:lst-ctn-kix_vnleep2i0q00-2 0}.lst-kix_50nrsf1v7avc-8>li:before{content:"-  "}.lst-kix_vnleep2i0q00-5>li{counter-increment:lst-ctn-kix_vnleep2i0q00-5}.lst-kix_50nrsf1v7avc-6>li:before{content:"-  "}.lst-kix_wgrv86mrt3st-0>li:before{content:"-  "}.lst-kix_wgrv86mrt3st-4>li:before{content:"-  "}.lst-kix_wgrv86mrt3st-2>li:before{content:"-  "}.lst-kix_ek9djv3fjw0j-7>li:before{content:"-  "}.lst-kix_ek9djv3fjw0j-5>li:before{content:"-  "}ol.lst-kix_z664cauv64el-7.start{counter-reset:lst-ctn-kix_z664cauv64el-7 0}.lst-kix_50nrsf1v7avc-0>li:before{content:"-  "}ul.lst-kix_h5e7jei2si3h-0{list-style-type:none}.lst-kix_vnleep2i0q00-0>li{counter-increment:lst-ctn-kix_vnleep2i0q00-0}ul.lst-kix_h5e7jei2si3h-1{list-style-type:none}ul.lst-kix_h5e7jei2si3h-2{list-style-type:none}ol.lst-kix_vnleep2i0q00-8{list-style-type:none}ol.lst-kix_vnleep2i0q00-7{list-style-type:none}ol.lst-kix_vnleep2i0q00-4{list-style-type:none}ol.lst-kix_vnleep2i0q00-3{list-style-type:none}ol.lst-kix_vnleep2i0q00-6{list-style-type:none}ol.lst-kix_vnleep2i0q00-5{list-style-type:none}ol.lst-kix_vnleep2i0q00-0{list-style-type:none}ol.lst-kix_vnleep2i0q00-2{list-style-type:none}ul.lst-kix_h5e7jei2si3h-7{list-style-type:none}ul.lst-kix_h5e7jei2si3h-8{list-style-type:none}.lst-kix_vnleep2i0q00-6>li{counter-increment:lst-ctn-kix_vnleep2i0q00-6}ul.lst-kix_h5e7jei2si3h-3{list-style-type:none}ul.lst-kix_h5e7jei2si3h-4{list-style-type:none}ol.lst-kix_vnleep2i0q00-4.start{counter-reset:lst-ctn-kix_vnleep2i0q00-4 0}ul.lst-kix_h5e7jei2si3h-5{list-style-type:none}ul.lst-kix_h5e7jei2si3h-6{list-style-type:none}.lst-kix_c0z30dja5xqc-7>li{counter-increment:lst-ctn-kix_c0z30dja5xqc-7}ol.lst-kix_c0z30dja5xqc-6.start{counter-reset:lst-ctn-kix_c0z30dja5xqc-6 0}.lst-kix_knhw06tmod61-7>li:before{content:"-  "}.lst-kix_knhw06tmod61-8>li:before{content:"-  "}.lst-kix_z664cauv64el-5>li{counter-increment:lst-ctn-kix_z664cauv64el-5}ol.lst-kix_vnleep2i0q00-6.start{counter-reset:lst-ctn-kix_vnleep2i0q00-6 0}.lst-kix_knhw06tmod61-2>li:before{content:"-  "}.lst-kix_knhw06tmod61-3>li:before{content:"-  "}.lst-kix_7se36hd4r36y-0>li:before{content:"-  "}.lst-kix_7se36hd4r36y-1>li:before{content:"-  "}.lst-kix_7se36hd4r36y-2>li:before{content:"-  "}.lst-kix_knhw06tmod61-6>li:before{content:"-  "}.lst-kix_knhw06tmod61-4>li:before{content:"-  "}.lst-kix_knhw06tmod61-5>li:before{content:"-  "}.lst-kix_7se36hd4r36y-3>li:before{content:"-  "}.lst-kix_7se36hd4r36y-6>li:before{content:"-  "}ul.lst-kix_50nrsf1v7avc-0{list-style-type:none}ul.lst-kix_50nrsf1v7avc-1{list-style-type:none}ul.lst-kix_50nrsf1v7avc-2{list-style-type:none}.lst-kix_7se36hd4r36y-4>li:before{content:"-  "}.lst-kix_7se36hd4r36y-8>li:before{content:"-  "}.lst-kix_7se36hd4r36y-5>li:before{content:"-  "}ol.lst-kix_z664cauv64el-5.start{counter-reset:lst-ctn-kix_z664cauv64el-5 0}ol.lst-kix_c0z30dja5xqc-0.start{counter-reset:lst-ctn-kix_c0z30dja5xqc-0 0}.lst-kix_h5e7jei2si3h-1>li:before{content:"-  "}.lst-kix_c0z30dja5xqc-5>li{counter-increment:lst-ctn-kix_c0z30dja5xqc-5}.lst-kix_knhw06tmod61-0>li:before{content:"-  "}.lst-kix_knhw06tmod61-1>li:before{content:"-  "}.lst-kix_h5e7jei2si3h-0>li:before{content:"-  "}.lst-kix_7se36hd4r36y-7>li:before{content:"-  "}.lst-kix_vnleep2i0q00-2>li{counter-increment:lst-ctn-kix_vnleep2i0q00-2}ul.lst-kix_50nrsf1v7avc-7{list-style-type:none}ul.lst-kix_50nrsf1v7avc-8{list-style-type:none}ul.lst-kix_50nrsf1v7avc-3{list-style-type:none}ul.lst-kix_50nrsf1v7avc-4{list-style-type:none}ul.lst-kix_50nrsf1v7avc-5{list-style-type:none}ul.lst-kix_50nrsf1v7avc-6{list-style-type:none}ol.lst-kix_c0z30dja5xqc-8{list-style-type:none}.lst-kix_1oaclixybewz-0>li:before{content:"-  "}ol.lst-kix_c0z30dja5xqc-7{list-style-type:none}ol.lst-kix_c0z30dja5xqc-6{list-style-type:none}ol.lst-kix_c0z30dja5xqc-5{list-style-type:none}ol.lst-kix_c0z30dja5xqc-4{list-style-type:none}ol.lst-kix_c0z30dja5xqc-3{list-style-type:none}ol.lst-kix_c0z30dja5xqc-2{list-style-type:none}.lst-kix_wgrv86mrt3st-8>li:before{content:"-  "}.lst-kix_wgrv86mrt3st-7>li:before{content:"-  "}.lst-kix_1oaclixybewz-3>li:before{content:"-  "}.lst-kix_wgrv86mrt3st-5>li:before{content:"-  "}.lst-kix_1oaclixybewz-1>li:before{content:"-  "}.lst-kix_1oaclixybewz-2>li:before{content:"-  "}.lst-kix_wgrv86mrt3st-6>li:before{content:"-  "}.lst-kix_h5e7jei2si3h-2>li:before{content:"-  "}.lst-kix_vnleep2i0q00-4>li{counter-increment:lst-ctn-kix_vnleep2i0q00-4}ol.lst-kix_z664cauv64el-4.start{counter-reset:lst-ctn-kix_z664cauv64el-4 0}.lst-kix_h5e7jei2si3h-3>li:before{content:"-  "}.lst-kix_h5e7jei2si3h-4>li:before{content:"-  "}ol.lst-kix_c0z30dja5xqc-7.start{counter-reset:lst-ctn-kix_c0z30dja5xqc-7 0}ul.lst-kix_knhw06tmod61-3{list-style-type:none}ul.lst-kix_knhw06tmod61-2{list-style-type:none}.lst-kix_h5e7jei2si3h-5>li:before{content:"-  "}.lst-kix_h5e7jei2si3h-6>li:before{content:"-  "}ul.lst-kix_knhw06tmod61-1{list-style-type:none}ul.lst-kix_knhw06tmod61-0{list-style-type:none}ol.lst-kix_vnleep2i0q00-0.start{counter-reset:lst-ctn-kix_vnleep2i0q00-0 0}ul.lst-kix_knhw06tmod61-7{list-style-type:none}ul.lst-kix_knhw06tmod61-6{list-style-type:none}ul.lst-kix_knhw06tmod61-5{list-style-type:none}ol.lst-kix_vnleep2i0q00-7.start{counter-reset:lst-ctn-kix_vnleep2i0q00-7 0}ul.lst-kix_knhw06tmod61-4{list-style-type:none}ul.lst-kix_knhw06tmod61-8{list-style-type:none}.lst-kix_z664cauv64el-2>li:before{content:"" counter(lst-ctn-kix_z664cauv64el-2,lower-roman) ". "}.lst-kix_h5e7jei2si3h-7>li:before{content:"-  "}.lst-kix_h5e7jei2si3h-8>li:before{content:"-  "}.lst-kix_1oaclixybewz-7>li:before{content:"-  "}.lst-kix_1oaclixybewz-8>li:before{content:"-  "}.lst-kix_z664cauv64el-1>li:before{content:"" counter(lst-ctn-kix_z664cauv64el-1,lower-latin) ". "}.lst-kix_z664cauv64el-7>li{counter-increment:lst-ctn-kix_z664cauv64el-7}ol.lst-kix_c0z30dja5xqc-0{list-style-type:none}.lst-kix_1oaclixybewz-4>li:before{content:"-  "}.lst-kix_z664cauv64el-0>li:before{content:"" counter(lst-ctn-kix_z664cauv64el-0,decimal) ". "}.lst-kix_c0z30dja5xqc-8>li:before{content:"" counter(lst-ctn-kix_c0z30dja5xqc-8,lower-roman) ". "}.lst-kix_c0z30dja5xqc-7>li:before{content:"" counter(lst-ctn-kix_c0z30dja5xqc-7,lower-latin) ". "}.lst-kix_1oaclixybewz-5>li:before{content:"-  "}.lst-kix_1oaclixybewz-6>li:before{content:"-  "}.lst-kix_z664cauv64el-1>li{counter-increment:lst-ctn-kix_z664cauv64el-1}.lst-kix_c0z30dja5xqc-5>li:before{content:"" counter(lst-ctn-kix_c0z30dja5xqc-5,lower-roman) ". "}ul.lst-kix_2o2vvckau1te-8{list-style-type:none}ul.lst-kix_2o2vvckau1te-1{list-style-type:none}ul.lst-kix_2o2vvckau1te-0{list-style-type:none}ul.lst-kix_2o2vvckau1te-3{list-style-type:none}ul.lst-kix_2o2vvckau1te-2{list-style-type:none}ul.lst-kix_2o2vvckau1te-5{list-style-type:none}.lst-kix_c0z30dja5xqc-3>li:before{content:"" counter(lst-ctn-kix_c0z30dja5xqc-3,decimal) ". "}ul.lst-kix_2o2vvckau1te-4{list-style-type:none}ul.lst-kix_2o2vvckau1te-7{list-style-type:none}ul.lst-kix_2o2vvckau1te-6{list-style-type:none}.lst-kix_z664cauv64el-4>li:before{content:"" counter(lst-ctn-kix_z664cauv64el-4,lower-latin) ". "}ol.lst-kix_c0z30dja5xqc-2.start{counter-reset:lst-ctn-kix_c0z30dja5xqc-2 0}.lst-kix_vnleep2i0q00-8>li{counter-increment:lst-ctn-kix_vnleep2i0q00-8}.lst-kix_c0z30dja5xqc-1>li:before{content:"\0025cb   "}.lst-kix_z664cauv64el-8>li:before{content:"" counter(lst-ctn-kix_z664cauv64el-8,lower-roman) ". "}.lst-kix_z664cauv64el-6>li:before{content:"" counter(lst-ctn-kix_z664cauv64el-6,decimal) ". "}.lst-kix_50nrsf1v7avc-3>li:before{content:"-  "}.lst-kix_c0z30dja5xqc-0>li{counter-increment:lst-ctn-kix_c0z30dja5xqc-0}.lst-kix_50nrsf1v7avc-5>li:before{content:"-  "}.lst-kix_wgrv86mrt3st-3>li:before{content:"-  "}.lst-kix_50nrsf1v7avc-7>li:before{content:"-  "}.lst-kix_wgrv86mrt3st-1>li:before{content:"-  "}.lst-kix_z664cauv64el-2>li{counter-increment:lst-ctn-kix_z664cauv64el-2}.lst-kix_ek9djv3fjw0j-8>li:before{content:"-  "}ol.lst-kix_c0z30dja5xqc-4.start{counter-reset:lst-ctn-kix_c0z30dja5xqc-4 0}.lst-kix_ek9djv3fjw0j-6>li:before{content:"-  "}.lst-kix_z664cauv64el-8>li{counter-increment:lst-ctn-kix_z664cauv64el-8}ol.lst-kix_z664cauv64el-0{list-style-type:none}ol.lst-kix_z664cauv64el-1{list-style-type:none}.lst-kix_ek9djv3fjw0j-4>li:before{content:"-  "}ol.lst-kix_z664cauv64el-2{list-style-type:none}ol.lst-kix_z664cauv64el-3{list-style-type:none}ol.lst-kix_z664cauv64el-4{list-style-type:none}ul.lst-kix_vnleep2i0q00-1{list-style-type:none}ol.lst-kix_z664cauv64el-5{list-style-type:none}ol.lst-kix_z664cauv64el-6{list-style-type:none}ol.lst-kix_z664cauv64el-7{list-style-type:none}ol.lst-kix_z664cauv64el-8{list-style-type:none}ol.lst-kix_z664cauv64el-1.start{counter-reset:lst-ctn-kix_z664cauv64el-1 0}.lst-kix_z664cauv64el-3>li{counter-increment:lst-ctn-kix_z664cauv64el-3}.lst-kix_50nrsf1v7avc-1>li:before{content:"-  "}ol.lst-kix_c0z30dja5xqc-5.start{counter-reset:lst-ctn-kix_c0z30dja5xqc-5 0}.lst-kix_vnleep2i0q00-3>li{counter-increment:lst-ctn-kix_vnleep2i0q00-3}ul.lst-kix_7se36hd4r36y-7{list-style-type:none}ul.lst-kix_7se36hd4r36y-8{list-style-type:none}ul.lst-kix_7se36hd4r36y-5{list-style-type:none}ul.lst-kix_7se36hd4r36y-6{list-style-type:none}ul.lst-kix_7se36hd4r36y-3{list-style-type:none}ul.lst-kix_7se36hd4r36y-4{list-style-type:none}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}ul.lst-kix_7se36hd4r36y-1{list-style-type:none}ul.lst-kix_7se36hd4r36y-2{list-style-type:none}ul.lst-kix_7se36hd4r36y-0{list-style-type:none}ol.lst-kix_z664cauv64el-0.start{counter-reset:lst-ctn-kix_z664cauv64el-0 0}ol{margin:0;padding:0}table td,table th{padding:0}.c21{background-color:#ffffff;margin-left:108pt;padding-top:12pt;padding-left:0pt;padding-bottom:12pt;line-height:1.0791666666666666;orphans:2;widows:2;text-align:left}.c16{background-color:#ffffff;margin-left:72pt;padding-top:14pt;padding-left:0pt;padding-bottom:0pt;line-height:1.0791666666666666;orphans:2;widows:2;text-align:left}.c23{background-color:#ffffff;margin-left:72pt;padding-top:12pt;padding-left:0pt;padding-bottom:12pt;line-height:1.0791666666666666;orphans:2;widows:2;text-align:left}.c12{background-color:#ffffff;padding-top:16pt;text-indent:36pt;padding-bottom:12pt;line-height:1.0791666666666666;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c27{background-color:#ffffff;padding-top:0pt;padding-bottom:12pt;line-height:1.0791666666666666;orphans:2;widows:2;text-align:left;height:11pt}.c52{background-color:#ffffff;padding-top:0pt;padding-bottom:7.5pt;line-height:1.0;orphans:2;widows:2;text-align:left;height:11pt}.c1{margin-left:36pt;padding-top:0pt;padding-left:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c22{margin-left:72pt;padding-top:12pt;padding-left:0pt;padding-bottom:12pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c36{background-color:#ffffff;padding-top:7.5pt;padding-bottom:14pt;line-height:1.0;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c46{background-color:#ffffff;padding-top:14pt;padding-bottom:0pt;line-height:1.0791666666666666;orphans:2;widows:2;text-align:left}.c50{background-color:#ffffff;padding-top:7.5pt;padding-bottom:14pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c62{padding-top:18pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c19{color:#333333;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10.5pt;font-family:"Arial";font-style:normal}.c2{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c7{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Calibri";font-style:normal}.c3{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left;height:11pt}.c31{padding-top:16pt;padding-bottom:4pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c5{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:normal}.c41{padding-top:15pt;padding-bottom:15pt;line-height:1.3636363636363635;orphans:2;widows:2;text-align:center;height:11pt}.c4{margin-left:36pt;padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c0{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c10{margin-left:36pt;padding-top:12pt;padding-bottom:12pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c25{background-color:#ffffff;padding-top:14pt;padding-bottom:4pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c14{background-color:#ffffff;padding-top:14pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c65{padding-top:14pt;padding-bottom:4pt;line-height:1.15;orphans:2;widows:2;text-align:left;height:14pt}.c40{background-color:#f7f7f7;padding-top:0pt;padding-bottom:0pt;line-height:1.0791666666666666;orphans:2;widows:2;text-align:left}.c64{padding-top:12pt;padding-bottom:8pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c32{padding-top:0pt;padding-bottom:8pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c15{color:#000000;text-decoration:none;vertical-align:baseline;font-size:13pt;font-family:"Arial";font-style:normal}.c71{font-weight:400;text-decoration:none;vertical-align:baseline;font-size:13.5pt;font-family:"Arial";font-style:normal}.c54{padding-top:24pt;padding-bottom:6pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c6{padding-top:0pt;padding-bottom:8pt;line-height:1.0791666666666666;orphans:2;widows:2;text-align:left}.c13{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c24{padding-top:14pt;padding-bottom:4pt;line-height:1.3636363636363635;orphans:2;widows:2;text-align:left}.c17{color:#000000;text-decoration:none;vertical-align:baseline;font-size:10.5pt;font-family:"Helvetica Neue";font-style:normal}.c51{padding-top:12pt;padding-bottom:12pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c45{padding-top:0pt;padding-bottom:12pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c72{color:#434343;text-decoration:none;vertical-align:baseline;font-size:15pt;font-family:"Arial";font-style:normal}.c66{padding-top:14pt;padding-bottom:4pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c35{color:#333333;text-decoration:none;vertical-align:baseline;font-size:13pt;font-family:"Arial";font-style:normal}.c33{padding-top:12pt;padding-bottom:12pt;line-height:1.3636363636363635;orphans:2;widows:2;text-align:left}.c73{font-weight:400;text-decoration:none;vertical-align:baseline;font-size:13.5pt;font-family:"Times New Roman";font-style:normal}.c69{text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Arial";font-style:normal}.c20{color:#000000;text-decoration:none;vertical-align:baseline;font-size:10.5pt;font-style:normal}.c47{font-weight:400;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c74{text-decoration:none;vertical-align:baseline;font-size:23pt;font-family:"Arial";font-style:normal}.c37{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c49{text-decoration:none;vertical-align:baseline;font-size:11pt;font-style:normal}.c76{text-decoration:none;vertical-align:baseline;font-style:normal}.c57{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;text-decoration:underline}.c28{font-family:"Roboto Mono";color:#188038;font-weight:400}.c48{font-size:7pt;font-family:"Times New Roman";font-weight:400}.c58{font-family:"Helvetica Neue";color:#333333;font-weight:400}.c26{font-size:15pt;color:#333333}.c42{color:inherit;text-decoration:inherit}.c68{font-family:"Roboto Mono";color:#188038}.c11{font-weight:400;font-family:"Courier New"}.c18{padding:0;margin:0}.c70{max-width:468pt;padding:72pt 72pt 72pt 72pt}.c8{font-weight:400;font-family:"Calibri"}.c43{text-indent:36pt}.c44{color:#0000ff}.c29{font-family:"Calibri"}.c59{color:#333333}.c75{font-size:13.5pt}.c39{margin-left:36pt}.c53{margin-left:72pt}.c38{padding-left:0pt}.c55{background-color:#ffffff}.c67{color:#af00db}.c34{color:#000000}.c9{font-weight:700}.c30{font-size:10.5pt}.c61{height:11pt}.c56{font-style:italic}.c60{color:#098658}.c63{font-family:"Helvetica Neue"}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c55 c70 doc-content"><h1 class="c54" id="h.gvre8vyqu2px"><span class="c59 c9 c74">Leaf Disease Detection and Classification</span></h1><p class="c33"><span class="c9 c26">Souvik Pramanik, Arnav Taya, Jefferson Chesson<br></span><span class="c59 c30">&nbsp;</span><span class="c59 c75">Fall 2023 ECE 4554/5554 Computer Vision: Course Project<br></span><span class="c30 c59">&nbsp;</span><span class="c59 c71">Virginia Tech</span></p><hr><p class="c41"><span class="c19"></span></p><h3 class="c66" id="h.9aemjv4rtqkw"><span class="c35 c9">Abstract</span></h3><p class="c33"><span class="c59 c30">In the biological field, diagnosis of medical conditions is very important to the efficacy of the treatments needed. Similarly in farming and agriculture, detection of leaf diseases in plants is critical to prevent spread to provide a better yield. Through the use of computer vision and Deep Convolutional Neural Network (CNN/ConvNet) techniques, many leaf diseases can be reliably detected, thus decreasing cost, time to treatment, and the risk of a misdiagnosis. CNN is able to successfully capture the Spatial and Temporal dependencies in an image through the application of relevant layers. Through the use of computer vision concepts learned in the class, combined with knowledge of deep convolutional neural networks and transfer learning using VGG16, ResNet50 and MonileNetV2, we hope to develop a program that will analyze images of leaves and determine whether the leaf is healthy or diseased. A web based application is also deployed regarding the same.<br><br></span><span class="c15 c9">Teaser Figure</span></p><p class="c33"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 578.50px; height: 329.33px;"><img alt="" src="images/image4.png" style="width: 578.50px; height: 329.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c33"><span class="c59 c30"><br></span><span class="c15 c9">Introduction</span></p><p class="c33"><span class="c2">One of the major reasons behind low crop production is due to bacteria, virus, and fungal plant diseases. Early detection of plant diseases and proper usage of pesticides and fertilizers are vital for preventing diseases and boost yield. Most farmers use generalized pesticides and fertilizers in entire fields without specifically knowing the condition of the plants. Therefore, it&rsquo;s very important to detect plant diseases without human interaction. Machine Learning models can be used to automatically predict plant diseases from images of diseased and healthy leaves.</span></p><h3 class="c24" id="h.k9a03l8hzs4"><span class="c9 c15">Dataset</span></h3><p class="c13"><span class="c2">For this project, we used the Kaggle dataset for Leaf Diseases[3] which has 70295 images belonging to 38 classes for training and 17572 images belonging to 38 classes for testing [5].&nbsp;The dataset is organized into train test folders with 38 directories corresponding to 38 classes. In the 38 classes for training there are between 1500 to more than 2000 images. In the 38 classes for testing there are between 400 &ndash; 500 images.The original dimension of the images are 256x256, however, some implementations below have reduced that to 128x128 for increased computational performance.</span></p><p class="c14" id="h.gjdgxs"><span class="c55 c9 c30 c63">The text editor used will be Google Colab. The needed libraries will be installed to directly extract data from Kaggle [2].</span></p><p class="c40"><span class="c11 c67 c30">from</span><span class="c11 c30">&nbsp;google.colab </span><span class="c11 c30 c67">import</span><span class="c20 c11">&nbsp;files</span></p><p class="c40"><span class="c20 c11">files.upload()</span></p><p class="c40"><span class="c11 c44 c30">!</span><span class="c20 c11">ls -lha kaggle.json</span></p><p class="c40"><span class="c11 c30 c44">!</span><span class="c20 c11">pip install -q kaggle</span></p><p class="c40"><span class="c11 c44 c30">!</span><span class="c11 c20">&nbsp;mkdir ~/.kaggle</span></p><p class="c40"><span class="c11 c44 c30">!</span><span class="c20 c11">cp kaggle.json ~/.kaggle/</span></p><p class="c40"><span class="c11 c44 c30">!</span><span class="c11 c30">chmod </span><span class="c11 c30 c60">600</span><span class="c20 c11">&nbsp;~/.kaggle/kaggle.json</span></p><p class="c14"><span class="c17 c9 c55">Dataset will be uploaded using the .json file. The Kaggle dataset will be downloaded using the API command below [2].</span></p><p class="c40"><span class="c11 c44 c30">!</span><span class="c11 c30">kaggle datasets download -d dev523/leaf-disease-detection-dataset</span></p><h3 class="c24" id="h.6yu7yrg9uwgz"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 497.50px; height: 522.16px;"><img alt="" src="images/image21.png" style="width: 497.50px; height: 522.16px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></h3><h3 class="c24" id="h.zgeq9ibk8tzu"><span class="c35 c9">Approach </span></h3><h3 class="c24" id="h.x5bdwktsffjb"><span class="c19">Most of the Leaf disease classification models are based on particular classes of species of leaves. In [1], 3 classes of plant species were used (potato, tomato, and pepper bell). &nbsp;In [6], predictions are made for tomato leaves. [8] focuses on diagnosing soybean plant diseases. Our approach is to generalize the prediction process to multiple classes of species so that it becomes an effective web-based tool for farmers and agriculturists. Our goal is to import more species of leaves to generalize the detection process. To improve the efficiency of the model, shifting of the layers can also be looked at [1].</span></h3><p class="c3"><span class="c2"></span></p><p class="c13"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 377.33px;"><img alt="" src="images/image11.png" style="width: 624.00px; height: 377.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3"><span class="c2"></span></p><h3 class="c31" id="h.ybzl73o9t685"><span class="c15 c9">CNN Model Concept</span></h3><p class="c3"><span class="c2"></span></p><p class="c13"><span class="c2">The approach of our project is to use different CNN models and different machine learning libraries for image classification. The libraries used are PyTorch and Keras Tensorflow and models implemented are CNN and CNN with ResNet. In this project, we implemented a Leaf Disease classification using CNN with Keras Tensorflow, CNN with PyTorch, and CNN with ResNet Architecture using PyTorch. These architectures are used for developing and training neural network-based models [8].&nbsp;&nbsp;PyTorch implementation is based on the image classification completed using Tensorflow.</span></p><p class="c3"><span class="c2"></span></p><p class="c45"><span class="c9">Convolutional Layer</span><span class="c2">: Responsible for detecting features in the input data through convolution operations, helping the network learn hierarchical representations.</span></p><p class="c51"><span class="c9">Batch Layer</span><span class="c2">: Used for normalizing the input data by adjusting and scaling the activations. This improves the stability and speed of training.</span></p><p class="c51"><span class="c9">Max Pooling Laye</span><span class="c2">r: Reduces the spatial dimensions of the input data by selecting the maximum value from a group of neighboring pixels. This helps in reducing computational complexity and extracting dominant features.</span></p><p class="c51"><span class="c9">ReLU</span><span class="c2">&nbsp;(Rectified Linear Unit): An activation function applied to introduce non-linearity in the model. It replaces all negative pixel values with zero, helping the network learn complex patterns.</span></p><p class="c13"><span class="c9">Cross Entropy</span><span class="c2">:</span></p><p class="c13"><span class="c2">Binary cross entropy was the loss function used. The actual value is multiplied by the log of the predicted value. This value is summed for all points in n. This sum is then divided by the number of points n to give the mean. Due to properties of logs however, the negative of the final function must be used to make the mean positive.</span></p><p class="c46"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 201.60px; height: 48.00px;"><img alt="" src="images/image8.png" style="width: 201.60px; height: 48.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c52"><span class="c59 c9 c30 c63 c76"></span></p><ol class="c18 lst-kix_z664cauv64el-0 start" start="1"><li class="c31 c39 c38 li-bullet-0"><h3 id="h.maz62kxzn42u" style="display:inline"><span class="c15 c9">Souvik Pramanik&rsquo;s Implementation</span></h3></li></ol><p class="c3"><span class="c2"></span></p><p class="c13"><span class="c2">Like checking whether a person is healthy, checking whether a plant is healthy is the first step to identifying a leaf disease. Other papers make specific categorizations of leaf diseases ([1], [5], [6], [7], [8]). This model detects whether the plant has a leaf disease. This is utilized with a binary classification model by sorting plants based on whether they are healthy or unhealthy.</span></p><p class="c13"><span class="c2">A CNN is a class of neural networks, defined as multilayered neural networks designed for computer vision applications to detect complex features in data. In the Pytorch model, there are 3 CNN blocks, and each block consists of a convolution layer, batch layer, max-pooling layer, and ReLU layer. ReLU activation function is used to remove negative values from the feature map because there cannot be negative values for any pixel value. Stride denotes how many steps are moving through each convolution. Padding is a process of adding zeros to the input matrix symmetrically and is used to maintain the same output and input dimensions [11]. In this model, Stride (1,1) is used with no padding involved. After applying convolution and extracting features from the image, a flatten layer is used to flatten the tensor which has 3 dimensions and converts to 1-D. The model was implemented using PyTorch using a basic CNN architecture as shown in the screenshot below.</span></p><p class="c13"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 392.00px;"><img alt="" src="images/image17.jpg" style="width: 624.00px; height: 392.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><ul class="c18 lst-kix_knhw06tmod61-0 start"><li class="c46 c39 c38 li-bullet-0"><span class="c9 c30 c63">Training criteria and results</span><span class="c59 c9 c30 c63"><br></span><span class="c30 c58"><br></span><span class="c29 c9">Batch size:</span><span class="c8">&nbsp;16; </span><span class="c29 c9">target_size:</span><span class="c8">&nbsp;(256, 256); </span><span class="c29 c9">Number of CNN layers:</span><span class="c8">&nbsp;3; </span><span class="c29 c9">Number of Epochs:</span><span class="c8">&nbsp;9; </span><span class="c29 c9">Training and Test samples: </span><span class="c8">1500; </span><span class="c29 c9">RunTime Type</span><span class="c7">: TPU</span></li></ul><p class="c6 c61"><span class="c7"></span></p><p class="c6 c43"><span class="c29 c9">Training Accuracy :</span><span class="c7">&nbsp;99.73</span></p><p class="c6 c43"><span class="c29 c9">Testing Accuracy &nbsp;</span><span class="c7">: 95.0</span></p><p class="c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 524.50px; height: 402.62px;"><img alt="" src="images/image14.png" style="width: 524.50px; height: 402.62px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c13"><span>&middot;</span><span class="c48">&nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c0">Prediction Comparison: </span></p><p class="c3"><span class="c0"></span></p><p class="c32 c53"><span class="c11">o</span><span class="c48">&nbsp; &nbsp;</span><span class="c7">Training Set Comparison</span></p><p class="c32"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 137.33px;"><img alt="" src="images/image20.png" style="width: 624.00px; height: 137.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c32 c53"><span class="c11">o</span><span class="c48">&nbsp; &nbsp;</span><span class="c7">Test Set Comparison</span></p><p class="c32"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 120.00px;"><img alt="" src="images/image9.png" style="width: 624.00px; height: 120.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c64"><span>&middot;</span><span class="c48">&nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c0">Test of individual leaf:</span></p><p class="c32 c53"><span class="c11">o</span><span class="c48">&nbsp; &nbsp;</span><span class="c7">Training Set Comparison</span></p><p class="c32"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 112.00px;"><img alt="" src="images/image16.png" style="width: 624.00px; height: 112.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c32 c53"><span class="c11">o</span><span class="c48">&nbsp; &nbsp;</span><span class="c7">Testing Set Comparison</span></p><p class="c32"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 156.00px;"><img alt="" src="images/image26.png" style="width: 624.00px; height: 156.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c13"><span class="c2">The model takes advantage of the power of CPU&rsquo;s (Central Processing Unit), GPU&rsquo;s (Graphical Processing Units), and TPU&rsquo;s (Tensor Processing Unit) in Google Colab. Graphical Processing Units (GPUs) provided on Google Colab to train a neural network faster than training a network on a CPU.</span></p><p class="c13"><span class="c2">90-99% accuracies were obtained for this binary classification model which was in line with results provided from other leaf disease implementations as presented below and in [1], &nbsp;[5], [6], [7], and [8]. While a validation dataset was not provided, a validation dataset was made using sklearn&rsquo;s train_test_split function. However, the training and testing accuracies ended up decreasing or lead to overfitting and underfitting. While it&rsquo;s important to have a validation dataset to identify and learn from data, this model did not utilize a validation dataset due to the decreasing accuracies it provided. &nbsp;</span></p><p class="c13"><span class="c2">PyTorch implementation of CNN model can be improved by working in the VT ARC (Advanced Research Computing) or similar computing environments which would help in utilizing the increased number of GPUs. While a Google Colab environment does contain GPU and TPU features, they come with a limitation that they can be only used for a certain number of hours at which the runtime type is switched back to CPU.</span></p><ol class="c18 lst-kix_z664cauv64el-0" start="2"><li class="c31 c39 c38 li-bullet-0"><h3 id="h.s0xgt5bsbrp" style="display:inline"><span class="c15 c9">Arnav Taya&rsquo;s Implementation</span></h3></li></ol><p class="c3"><span class="c2"></span></p><p class="c13"><span>Three different convolutional neural network (CNN) architectures&mdash; baseline CNN, VGG16 [9], and MobileNetV2 [10]&mdash;were employed to classify leaf diseases in a dataset. The models were evaluated based on two crucial metrics: loss and accuracy. The results offer insights into the performance of each model in addressing the leaf disease classification task.</span></p><p class="c3"><span class="c2"></span></p><ul class="c18 lst-kix_7se36hd4r36y-0 start"><li class="c1 li-bullet-0"><span class="c9">Preprocessing</span><span class="c2">: The images in the dataset are first reduced to half the size i.e. 128x128 and further divided into training set(56,251 images), validation set(14,044 images) and testing set(17,572 images).</span></li></ul><p class="c4"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 352.00px;"><img alt="" src="images/image3.png" style="width: 624.00px; height: 352.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3"><span class="c2"></span></p><ul class="c18 lst-kix_wgrv86mrt3st-0 start"><li class="c1 li-bullet-0"><span class="c0">Baseline CNN model</span></li></ul><p class="c4"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 324.00px;"><img alt="" src="images/image18.png" style="width: 624.00px; height: 324.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3 c39"><span class="c2"></span></p><ul class="c18 lst-kix_wgrv86mrt3st-0"><li class="c1 li-bullet-0"><span class="c9">VGG16 [9] base for transfer learning model: </span><span class="c2">VGG16, short for Visual Geometry Group 16, is a convolutional neural network architecture designed for image classification. It was introduced by the Visual Geometry Group at the University of Oxford. VGG16 is known for its simplicity and uniform architecture, consisting of 16 weight layers, including 13 convolutional layers and 3 fully connected layers. The convolutional layers use small 3x3 filters with a stride of 1, and max pooling is applied after every two convolutional layers to reduce spatial dimensions. Despite its straightforward design, VGG16 achieved competitive performance on various image classification tasks and became a benchmark architecture.</span></li></ul><p class="c4"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 326.67px;"><img alt="" src="images/image19.png" style="width: 624.00px; height: 326.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3 c39"><span class="c2"></span></p><ul class="c18 lst-kix_wgrv86mrt3st-0"><li class="c1 li-bullet-0"><span class="c9">MobileNetV2 [10] base for transfer learning model: </span><span class="c2">MobileNetV2 is a lightweight convolutional neural network architecture specifically designed for mobile and edge devices with limited computational resources. It was developed by Google researchers as an improvement over the original MobileNet. MobileNetV2 utilizes depth wise separable convolutions to reduce the number of parameters and computations, making it more efficient for deployment on resource-constrained devices. It introduces inverted residuals and linear bottlenecks to enhance the learning capacity of the network while maintaining computational efficiency. MobileNetV2 is well-suited for applications such as image classification, object detection, and semantic segmentation on devices with low power and computational capabilities. Its efficiency makes it a popular choice for real-time, on-device computer vision tasks.</span></li></ul><p class="c4"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 330.67px;"><img alt="" src="images/image13.png" style="width: 624.00px; height: 330.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3 c39"><span class="c2"></span></p><ul class="c18 lst-kix_2o2vvckau1te-0 start"><li class="c1 li-bullet-0"><span class="c9">Training criteria and results</span><span class="c2">:</span></li></ul><ul class="c18 lst-kix_2o2vvckau1te-1 start"><li class="c16 li-bullet-0"><span class="c29 c9">Batch size:</span><span class="c8">&nbsp;32; </span><span class="c29 c9">input_size:</span><span class="c8">&nbsp;(128, 128); </span><span class="c29 c9">Steps per epoch:</span><span class="c8">&nbsp;200; </span><span class="c29 c9">Number of Epochs:</span><span class="c8">&nbsp;10; </span><span class="c29 c9">Validation steps: </span><span class="c8">100</span><span class="c8">; </span><span class="c29 c9">RunTime Type</span><span class="c7">: T4 GPU</span></li><li class="c16 li-bullet-0"><span class="c7">Training and validation loss and accuracy charts:</span></li></ul><p class="c46 c61"><span class="c7"></span></p><p class="c46 c61"><span class="c7"></span></p><p class="c46"><span class="c29 c9">CNN</span><span class="c8">&nbsp;(Validation accuracy: 84.48 , Testing accuracy: 88.06)</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 258.67px;"><img alt="" src="images/image15.png" style="width: 624.00px; height: 258.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c46 c61"><span class="c29 c34 c9 c49"></span></p><p class="c46"><span class="c9 c29">VGG16</span><span class="c8">&nbsp;(Validation accuracy: 89.27 , Testing accuracy: 91.57)</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 256.00px;"><img alt="" src="images/image12.png" style="width: 624.00px; height: 256.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c46"><span class="c29 c9">MobileNetV2</span><span class="c8">&nbsp;(Validation accuracy: 90.97 , Testing accuracy: 93.99)</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 256.00px;"><img alt="" src="images/image24.png" style="width: 624.00px; height: 256.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c46"><span class="c29 c9">Testing set</span><span class="c7">&nbsp;loss and accuracy(multiply by 100 to get percentage):</span></p><p class="c46"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 217.33px;"><img alt="" src="images/image29.png" style="width: 624.00px; height: 217.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c27"><span class="c49 c29 c34 c9"></span></p><h3 class="c12" id="h.43vkbigs33ex"><span class="c34">-</span><span class="c5">Key Findings:</span></h3><ol class="c18 lst-kix_c0z30dja5xqc-0 start" start="1"><li class="c23 li-bullet-0"><span class="c0">Model Hierarchy:</span></li></ol><ul class="c18 lst-kix_c0z30dja5xqc-1 start"><li class="c21 li-bullet-0"><span class="c2">MobileNetV2, with its efficient architecture, outshone the traditional CNN and VGG16 models in leaf disease classification.</span></li><li class="c21 li-bullet-0"><span class="c2">VGG16, a deeper model, performed better than the baseline CNN, indicating the potential benefits of a more complex architecture.</span></li></ul><ol class="c18 lst-kix_c0z30dja5xqc-0" start="2"><li class="c23 li-bullet-0"><span class="c0">Trade-offs:</span></li></ol><ul class="c18 lst-kix_c0z30dja5xqc-1 start"><li class="c21 li-bullet-0"><span class="c2">While VGG16 showed a better accuracy than the baseline CNN, it came at the cost of a slightly increased computational load.</span></li><li class="c21 li-bullet-0"><span>MobileNetV2 demonstrated superior efficiency with both lower loss and higher accuracy, making it a suitable choice for applications where computational resources are limited.</span><span class="c2">.</span></li></ul><h3 class="c31" id="h.azf2tobne8hb"><span class="c15 c9">3. Jefferson Chesson&rsquo;s Implementation</span></h3><p class="c3"><span class="c2"></span></p><p class="c13"><span>The </span><span class="c9">ResNet</span><span class="c2">&nbsp;architecture [11] is a CNN model that makes use of residual blocks to retain values through convolution. This prevents the problem of a vanishing gradient that sometimes occurs when an input goes through many layers within a network. The ResNet residual blocks hold values that have not gone through certain layers so that those values can be used in later layers that would usually only receive the output of the previous layer. The ResNet can be designed in different ways that each use a different number of filters in their layers. We tested both the 34 and 152 layer implementations since they were the most complex of the designs that used 2 filters per layer and 3 filters per layer respectively. The models were designed from scratch using the blog from Nouman [12] and the GitHub repository by JayPatwardhan [13].</span></p><p class="c13"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 277.33px;"><img alt="" src="images/image6.jpg" style="width: 624.00px; height: 277.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3"><span class="c2"></span></p><ul class="c18 lst-kix_h5e7jei2si3h-0 start"><li class="c1 li-bullet-0"><span class="c2">After downloading the dataset from Kaggle, which was already split into testing and training data, the data was transformed so that it could be used in the model. The ResNet only takes 224x224 images as inputs so the images had to be resized from their original size of 256x256. The images also had to be converted to a tensor, which is a multidimensional matrix that PyTorch uses to hold data. Finally, the images had to be normalized to a similar range used for the weights and biases. The testing images did not need to be normalized since they wouldn&rsquo;t be used to tune parameters but they did need to be resized and converted to tensors. Using the ImageFolder method from PyTorch, the training and testing images were automatically read from their folders into datasets and the names of the subfolders were used as class labels for a total of 38 classes. Using a random split method from PyTorch that also ensured each split contained examples from each class, the training dataset was split so that 30% of it was used as a validation set.</span></li></ul><p class="c13"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 413.33px;"><img alt="" src="images/image7.jpg" style="width: 624.00px; height: 413.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3"><span class="c2"></span></p><ul class="c18 lst-kix_1oaclixybewz-0 start"><li class="c1 li-bullet-0"><span class="c2">Each ResNet architecture uses 5 convolution layers along with an average pooling layer, max pooling layer, and fully connected output layers. Both implementations use the cross entropy loss function and use mini-batch stochastic gradient descent to optimize parameters with batch sizes of 64. The ResNet-50 only differs from the ResNet-152 in that there are 2 different convolution filters instead of 3 and the number of blocks per layer. The model was tested on the entire test dataset and the accuracy was obtained by recording how many label classifications it performed correctly.</span></li></ul><p class="c13"><span class="c2">ResNet-50 seemed to perform well on 5 epochs, regularly getting at least 94% accuracy on the testing set. Increasing to 10 epochs caused overfitting and dropped the testing accuracy to 70-80% while still increasing in validation accuracies. The learning rate was kept at around 0.01 because any lower or higher in magnitude caused a drop in testing accuracy and/or training time. The weight decay and momentum were kept at 0.001 and 0.9 respectively. These just happened to be the first parameters that were tried and consistently performed better in testing than other values when tuning. Due to the issues ResNet-50 had with overfitting at 10 epochs, I decided to try both 5 and 10 epochs for ResNet-152. The parameters were kept the same for consistency. The ResNet-152 architecture did not outperform ResNet-50 and even showed signs of overfitting as the epochs grew.</span></p><p class="c3"><span class="c2"></span></p><ul class="c18 lst-kix_ek9djv3fjw0j-0 start"><li class="c1 li-bullet-0"><span class="c9">Training criteria and results</span><span class="c2">:</span></li></ul><p class="c4"><span class="c9">Batch size</span><span>: 64; </span><span class="c9">Epochs</span><span>: 5; </span><span class="c9">Learning Rate</span><span>&nbsp;= 0.01; </span><span class="c9">Weight Decay</span><span>&nbsp;= 0.001; </span><span class="c9">Momentum </span><span>= 0.9; </span><span class="c9">RunTime Type</span><span class="c2">: T4 GPU</span></p><p class="c3"><span class="c2"></span></p><p class="c13"><span class="c9">ResNet50</span><span class="c2">&nbsp;output, and plots:</span></p><p class="c13"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 464.00px;"><img alt="" src="images/image10.png" style="width: 624.00px; height: 464.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c13"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 30.67px;"><img alt="" src="images/image27.jpg" style="width: 624.00px; height: 30.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3"><span class="c2"></span></p><p class="c13"><span class="c9">ResNet152</span><span class="c2">&nbsp;output, and plots:</span></p><p class="c13"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 465.33px;"><img alt="" src="images/image23.png" style="width: 624.00px; height: 465.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 33.33px;"><img alt="" src="images/image5.jpg" style="width: 624.00px; height: 33.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h2 class="c62" id="h.gj90nuu35ir3"><span class="c9 c72">Leaf image classifier Web Page: Workflow</span></h2><ol class="c18 lst-kix_vnleep2i0q00-0 start" start="1"><li class="c10 c38 li-bullet-0"><span class="c0">Flask Web App Setup in Google Colab:</span></li></ol><ul class="c18 lst-kix_vnleep2i0q00-1 start"><li class="c22 li-bullet-0"><span class="c2">Utilized Flask to create a simple web application.</span></li><li class="c22 li-bullet-0"><span class="c2">Integrated ngrok for temporary internet exposure of the Flask app in a Colab environment.</span></li><li class="c22 li-bullet-0"><span>Created an HTML template (</span><span class="c28">index.html</span><span class="c2">) for the web page.</span></li></ul><ol class="c18 lst-kix_vnleep2i0q00-0" start="2"><li class="c10 c38 li-bullet-0"><span class="c0">Model Loading and Prediction Handling:</span></li></ol><p class="c10"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 573.50px; height: 274.58px;"><img alt="" src="images/image22.png" style="width: 573.50px; height: 274.58px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><ul class="c18 lst-kix_vnleep2i0q00-1"><li class="c22 li-bullet-0"><span class="c2">Loaded a pre-trained MobileNetV2 model(from Arnav&rsquo;s implementation) for image classification.</span></li><li class="c22 li-bullet-0"><span class="c2">Modified the Flask code to handle image uploads and make predictions using the model.</span></li><li class="c22 li-bullet-0"><span class="c2">Initially encountered issues with prediction outputs being &quot;undefined.&quot;</span></li></ul><ol class="c18 lst-kix_vnleep2i0q00-0" start="3"><li class="c10 c38 li-bullet-0"><span class="c0">Custom Model Integration:</span></li></ol><ul class="c18 lst-kix_vnleep2i0q00-1 start"><li class="c22 li-bullet-0"><span class="c2">Currently designed for tensorflow and keras architecture. Further additions can be made to support PyTorch implementations.</span></li><li class="c22 li-bullet-0"><span>Highlighted the importance of saving the model using </span><span class="c28">model.save()</span><span>&nbsp;and choosing an appropriate file extension (e.g., </span><span class="c28">.h5</span><span class="c2">).</span></li></ul><ol class="c18 lst-kix_vnleep2i0q00-0" start="4"><li class="c10 c38 li-bullet-0"><span class="c0">Prediction Handling for Multi-Class Classification:</span></li></ol><ul class="c18 lst-kix_vnleep2i0q00-1 start"><li class="c22 li-bullet-0"><span class="c2">Adjusted the prediction handling code to accommodate models with multiple classes.</span></li><li class="c22 li-bullet-0"><span>Used </span><span class="c28">np.argmax(predictions, axis=1)</span><span class="c2">&nbsp;to identify the predicted class index.</span></li></ul><ol class="c18 lst-kix_vnleep2i0q00-0" start="5"><li class="c10 c38 li-bullet-0"><span class="c0">Debugging and Troubleshooting:</span></li></ol><ul class="c18 lst-kix_vnleep2i0q00-1 start"><li class="c22 li-bullet-0"><span class="c2">Emphasized the importance of checking model input size, normalization, and preprocessing consistency.</span></li><li class="c22 li-bullet-0"><span class="c2">Suggested debugging techniques such as printing raw predictions and checking class mappings.</span></li></ul><ol class="c18 lst-kix_vnleep2i0q00-0" start="6"><li class="c10 c38 li-bullet-0"><span class="c0">Class Mapping for Response:</span></li></ol><ul class="c18 lst-kix_vnleep2i0q00-1 start"><li class="c22 li-bullet-0"><span class="c2">Updated the Flask code to include a class mapping that converts numeric indices to class names in the response.</span></li></ul><ol class="c18 lst-kix_vnleep2i0q00-0" start="7"><li class="c10 c38 li-bullet-0"><span class="c9">Summary of Final </span><span class="c9 c68">/predict</span><span class="c0">&nbsp;Route:</span></li></ol><p class="c10"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 480.00px;"><img alt="" src="images/image2.png" style="width: 624.00px; height: 480.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><ul class="c18 lst-kix_vnleep2i0q00-1"><li class="c22 li-bullet-0"><span>The final </span><span class="c28">/predict</span><span class="c2">&nbsp;route preprocesses the uploaded image, makes predictions using a custom model, maps the predicted index to a class name, and returns the result in a JSON response.</span></li></ul><p class="c51"><span class="c0">Final webpage Output:</span></p><p class="c51"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 390.67px;"><img alt="" src="images/image1.png" style="width: 624.00px; height: 390.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 390.67px;"><img alt="" src="images/image25.png" style="width: 624.00px; height: 390.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 390.67px;"><img alt="" src="images/image28.png" style="width: 624.00px; height: 390.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h3 class="c65" id="h.8575d634k8ij"><span class="c9 c35"></span></h3><h3 class="c36" id="h.xgm47a0mvgh"><span class="c69 c34 c9">Conclusion</span></h3><p class="c13"><span>Leaf disease </span><span class="c2">Image classification by using CNN[4] model with three different architectures was implemented in this project. Souvik&rsquo;s implementation gave us an initial CNN application doing binary classification between healthy and unhealthy leaves. Arnav further used the CNN model and compared it with VGG16[9] and MobileNetV2[10] to classify into 38 classes provided by the dataset where MobileNetV2 emerged as the top-performing model, balancing accuracy and efficiency. Jefferson further compared two different versions of a very popular architecture ResNet[12] and compared their performance.</span></p><p class="c3"><span class="c2"></span></p><p class="c13"><span class="c2">Finally, a web page deployed on flask and ngrock framework was presented which takes any leaf image and outputs its classification. Currently the web page is designed to be used with Tensorflow and Keras but further additions can be made for PyTorch models.</span></p><p class="c3"><span class="c2"></span></p><p class="c13"><span class="c2">These findings contribute to the understanding of suitable model choices for similar tasks and lay the foundation for future research and application in the field of plant pathology.</span></p><h3 class="c25" id="h.izgmgi2enmrs"><span class="c34 c9 c69">References</span></h3><p class="c14"><span class="c2">[1] M. Shobana et al., &quot;Plant Disease Detection Using Convolution Neural Network,&quot; 2022 International Conference on Computer Communication and Informatics (ICCCI), Coimbatore, India, 2022, pp. 1-5, doi: 10.1109/ICCCI54379.2022.9740975.</span></p><p class="c13"><span>[2] </span><span class="c37"><a class="c42" href="https://www.google.com/url?q=https://towardsdatascience.com/medical-x-ray-%25EF%25B8%258F-image-classification-using-convolutional-neural-network-9a6d33b1c2a&amp;sa=D&amp;source=editors&amp;ust=1701220208748500&amp;usg=AOvVaw13q0RTV6_CPrO3-JcJyoxo">https://towardsdatascience.com/medical-x-ray-%EF%B8%8F-image-classification-using-convolutional-neural-network-9a6d33b1c2a</a></span></p><p class="c14"><span class="c2">[3] https://www.kaggle.com/datasets/dev523/leaf-disease-detection-dataset/data</span></p><p class="c14"><span class="c2">[4] https://towardsdatascience.com/medical-x-ray-%EF%B8%8F-image-classification-using-convolutional-neural-network-9a6d33b1c2a</span></p><p class="c14"><span class="c2">[5] Harakannanavar, S. S., Rudagi, J. M., Puranikmath, V. I., Siddiqua, A., and Pramodhini, R., &ldquo;Plant leaf disease detection using computer vision and machine learning algorithms&rdquo;, &lt;i&gt;Global Transitions Proceedings&lt;/i&gt;, vol. 3, no. 1, pp. 305&ndash;310, 2022. doi:10.1016/j.gltp.2022.03.016.</span></p><p class="c14"><span class="c2">[6] Rehana, H., Ibrahim, M., and Haider Ali, M., &ldquo;Plant Disease Detection using Region-Based Convolutional Neural Network&rdquo;, &lt;i&gt;arXiv e-prints&lt;/i&gt;, 2023. doi:10.48550/arXiv.2303.09063.</span></p><p class="c50"><span class="c34">[7] Jadhav, S.B., Udupi, V.R. &amp; Patil, S.B. Identification of plant diseases using convolutional neural networks. Int. j. inf.&nbsp;</span><span>technol.</span><span class="c34">&nbsp;13, 2461&ndash;2470 (2021). </span><span class="c44 c57"><a class="c42" href="https://www.google.com/url?q=https://doi.org/10.1007/s41870-020-00437-5&amp;sa=D&amp;source=editors&amp;ust=1701220208749226&amp;usg=AOvVaw2fHYVsZxbFPY5HeBcuUotX">https://doi.org/10.1007/s41870-020-00437-5</a></span></p><p class="c14"><span class="c2">[8] Jadhav, S.B., Udupi, V.R. &amp; Patil, S.B. Identification of plant diseases using convolutional neural networks. Int. j. inf. tecnol. 13, 2461&ndash;2470 (2021). https://doi.org/10.1007/s41870-020-00437-5</span></p><p class="c50"><span>[9] Simonyan, Karen, and Andrew Zisserman. &quot;Very deep convolutional networks for large-scale image recognition.&quot; </span><span class="c56">arXiv preprint arXiv:1409.1556</span><span class="c2">&nbsp;(2014).</span></p><p class="c13"><span>[10] Sandler, Mark, et al. &quot;Mobilenetv2: Inverted residuals and linear bottlenecks.&quot; </span><span class="c56">Proceedings of the IEEE conference on computer vision and pattern recognition</span><span class="c2">. 2018.</span></p><p class="c3"><span class="c2"></span></p><p class="c13"><span>[11] K. He, X. Zhang, S. Ren, and J. Sun, &ldquo;Deep Residual Learning for Image Recognition,&rdquo; arXiv.org, Dec. 10, 2015. </span><span class="c37"><a class="c42" href="https://www.google.com/url?q=https://arxiv.org/abs/1512.03385&amp;sa=D&amp;source=editors&amp;ust=1701220208749830&amp;usg=AOvVaw0xCIAueOKiypNFDsVsbRjs">https://arxiv.org/abs/1512.03385</a></span><span class="c2">.</span></p><p class="c3"><span class="c2"></span></p><p class="c13"><span class="c2">[12] Nouman. &ldquo;Writing ResNet from Scratch in PyTorch.&rdquo; Paperspace Blog, 21 June 2022, blog.paperspace.com/writing-resnet-from-scratch-in-pytorch/#future-work.</span></p><p class="c3"><span class="c2"></span></p><p class="c13"><span class="c2">[13] J. Patwardhan, &ldquo;ResNet-PyTorch,&rdquo; GitHub, https://github.com/JayPatwardhan/ResNet-PyTorch/tree/master</span></p><hr><p class="c41"><span class="c19"></span></p><p class="c13"><span>&copy; Souvik Pramanik, Arnav Taya, Jefferson Chesson</span></p></body></html>